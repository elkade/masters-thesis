\documentclass[12pt, twoside, openany]{report}
\usepackage[dvips]{graphicx,color,rotating}
\usepackage[cp1250]{inputenc}
\usepackage{t1enc}
\usepackage{a4wide}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{verbatim}
%\usepackage[MeX]{polski}
\usepackage[hyphenbreaks]{breakurl}
\usepackage[hyphens]{url}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{left=25mm,right=25mm,bindingoffset=10mm, top=25mm, bottom=25mm}
\usepackage{amssymb, latexsym}
\usepackage{amsthm}
\usepackage{palatino}
\usepackage{array}
\usepackage{pstricks}
\usepackage{textcomp}
\usepackage{float}
\usepackage[none]{hyphenat}
\usepackage[english, polish]{babel}
\usepackage{hyperref}
\theoremstyle{definition}
\newtheorem{theorem}{Twierdzenie}[section]
\newtheorem{remark}{Uwaga}[section]
\newtheorem{definition}{Definicja}[section]
\newtheorem{alg}{Algorytm}[chapter]
\newtheorem{prz}{Przypadek}[section]
\newtheorem{np}{Przyk≥ad}[section]
\newtheorem{lemma}[theorem]{Lemat}
\linespread{1.5}
\usepackage{indentfirst}
\newcommand*{\norm}[1]{\left\Vert{#1}\right\Vert}
\newcommand*{\abs}[1]{\left\vert{#1}\right\vert}
\newcommand*{\om}{\omega}
\newcommand{\myparagraph}[1]{\paragraph{#1}\mbox{}\\}
%\renewcommand{\floatpagefraction}{.99}%
\author{£ukasz Dragan}
\title{Rekomendacje artyku≥Ûw opisujπcych produkty w serwisach e-commerce}

\begin{document}
\sloppy
\hyphenpenalty 10000
\exhyphenpenalty 10000
\righthyphenmin 10000
\lefthyphenmin 10000
\hyphenchar\font=-1
% Za≥Û≥Ê gÍúlπ jaüÒ.
\begin{titlepage}
\pagestyle{empty}

\noindent
\begin{Large}
\begin{table}[t]
\centering
\begin{tabular}[t]{lcr}
 \includegraphics[width=70pt,height=70pt]{PW} & POLITECHNIKA WARSZAWSKA & \includegraphics[width=70pt,height=70pt]{MiNI}\\
& WYDZIA£ MATEMATYKI & \\
& I NAUK INFORMACYJNYCH &
\end{tabular}
\end{table}

\begin{center}PRACA DYPLOMOWA MAGISTERSKA\end{center}
\begin{center}INFORMATYKA\end{center}\end{Large}
\begin{center}
\Large
\textbf{Rekomendacje artyku≥Ûw opisujπcych produkty w serwisach e-commerce}
\vfill
\large
\textbf{Content-based recommendations in e-commerce services}
\vfill
\normalsize
Autor:\\
\LARGE
£ukasz Dragan
\vfill
\normalsize
Promotor: 
\large
dr inø. Anna WrÛblewska
\vfill
\large
Warszawa, wrzesieÒ 2017
\end{center}
\newpage
\hfill
\begin{table}[b]
\centering
\begin{tabular}[t]{ccc}
............................................. & \hspace*{100pt} & .............................................\\
podpis promotora & \hspace*{100pt} & podpis autora
\end{tabular}
\end{table}


% \maketitle
\end{titlepage}
\thispagestyle{empty}
\newpage
\pagestyle{headings}
\setcounter{page}{1}
\hyphenation{Syl-ves-tra}
\hyphenation{Syl-ves-ter-a}

\begin{abstract}%DONE
	
	Tematyka niniejszej pracy skupia siÍ wokÛ≥ zagadnieÒ okreúlania podobieÒstwa semantycznego pomiÍdzy dokumentami tekstowymi i rekomendacji dokumentÛw podobnych do danego. SzczegÛ≥owy problem pochodzi z internetowego serwisu aukcyjnego Allegro, ktÛry posiada dzia≥ artyku≥Ûw opisujπcych produkty dostÍpne w serwisie. W dziale tym funkcjonuje system rekomendacji podobnych artyku≥Ûw tekstowych w oparciu o ich treúÊ. Celem pracy jest zbadanie moøliwoúci usprawnienia dzia≥ania istniejπcego systemu rekomandacji wykorzystujπc metody semantycznej analizy tekstu.
	
	W niniejszej pracy adaptujÍ dostÍpne metody okreúlania podobieÒstwa pomiÍdzy dokumentami tekstowymi do powyøszego problemu, wprowadzam miary umoøliwiajπce ocenÍ dzia≥ania tych metod oraz dokonujÍ analizy moøliwoúci ich wykorzystania w rzeczywistym systemie.
\end{abstract}
\begin{otherlanguage}{english}
	\begin{abstract}
	The subject of this paper focuses on the issues of determining the semantic similarity between text documents and the recommendation of documents similar to a given. A detailed problem comes from the Allegro on-line auction site, which has a section of articles describing the products available on the site. This section offers a recommendation system for similar textual articles based on their content. The aim of this paper is to investigate the possibility of improving the existing recommendation system using semantic text analysis methods.
	
	In this paper, I adapt the available methods for determining the similarity between text documents to the above problem, I introduce measures to evaluate the performance of these methods and analyze the possibilities of using them in the real system.
	\end{abstract}
\end{otherlanguage}

\tableofcontents
\clearpage
%-----------Poczπtek czÍúci zasadniczej-----------

\chapter{WstÍp}%DONE

Systemy rekomendacji sπ powszechnym elementem wielu serwisÛw internetowych. Sprawdzajπ siÍ na takich polach, jak polecanie produktÛw w sklepie czy rekomendacje ofert pracy. Dajπ uøytkownikowi poczucie indywidualnego traktowania przez serwis internetowy dopasowujπcy niejako zawartoúÊ swoich stron do konkretnego uøytkownika. Pozwala to uøytkownikowi na bardziej efektywne korzystanie z serwisu oraz moøe prowadziÊ do wiÍkszego zaangaøowania ze strony uøytkownika i przywiπzania do serwisu. Systemy rekomendacji dajπ obopulnπ korzyúÊ zarÛwno uøytkownikowi jak i w≥aúcicielowi serwisu internetowego.

Celem niniejszej pracy magisterskiej jest analiza moøliwoúci usprawnienia istniejacego systemu rekomendacji o oparciu o adaptacjÍ istniejπcych metod wyszukiwania semantycznego podobieÒstwa pomiÍdzy dokumentami tekstowymi. Rzeczony system rekomendacji istnieje w internetowym serwisie e-commerce Allegro w dziale artyku≥Ûw tekstowych o tematyce zwiπzanej z produktami dostÍpnymi za poúrednictwem serwisu. System ma na celu zarekomendowanie uøytkownikowi artyku≥Ûw o tematyce podobnej do tego, ktÛry znajduje siÍ na stornie aktualnie odwiedzanej przez uzytkownika.

W swojej pracy badam moøliwoúÊ uøycia istniejπcych metod semantycznej analizy tekstu w odniesieniu do opisanego problemu. Badane metody to: Latent Semantic Analysis, Latent Dirichlet Allocation, Word2vec, GloVe oraz FastText. JakoúÊ dzia≥ania tych metod porÛwnujÍ poprzez samodzielnie opracowane metody ewaluacji.

Podczas prowadzenia badaÒ stworzy≥em szereg skryptÛw przetwarzajπcych dane i wykorzystujπcych implementacje opisywanych w tej pracy metod. Opis uøytych narzÍdzi programistycznych i bibliotek zawar≥em w dodatku A do niniejszej pracy.

\section{Rekomendacje artyku≥Ûw tekstowych w Allegro}%DONE
Allegro jest najwiÍkszπ\cite{all_naj} dzia≥ajπcπ na rynku polskim platformπ aukcyjnπ on-line. Posiada ponad 20 mln zarejestrowanych klientÛw. Kaødego dnia na Allegro sprzedaje siÍ ponad 870 tysiÍcy przedmiotÛw. Zatrudnia 1300 pracownikÛw.\cite{allegro} Serwis umoøliwia uøytkownikom wystawianie na sprzedaø oraz kupno przedmiotÛw poprzez mechanizm licytacji lub natychmiastowego zakupu.

OprÛcz g≥Ûwnej czÍúci serwisu odpowiedzialnej za transakcje Allegro posiada dzia≥ zajmujπcy siÍ publikacjπ artyku≥Ûw opisujπcych produkty wystawiane za poúrednictwem serwisu. Ma to na celu pomoc uøytkownikom przy wyborze interesujπcego ich produktu.

Po to, aby zachÍciÊ uøytkownikÛw do zapoznania siÍ z treúciπ kolejnych artyku≥Ûw, zastosowany zosta≥ tu system rekomendacji przyporzπdkowujπcy danemu artyku≥owi listÍ powiπzanych artyku≥Ûw. Kryterium mÛwiπcym, czy artyku≥y sπ powiπzane jest tutaj jedynie treúÊ samych artyku≥Ûw, a nie wczeúciejsze zachowanie uøytkownika.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{img/screen_allegro.png}
	\caption{Widok strony internetowej zawierajπcej jeden z artyku≥Ûw serwisu Allegro. \cite{screen_allegro}}
\end{figure}

Od serwisu Allegro otrzyma≥em zserializowanπ kopiÍ 20000 artyku≥Ûw dostÍpnych na stronach serwisu. Pojedynczy artyku≥ sk≥ada siÍ z g≥Ûwnej zawartoúci tekstowej oraz metadanych. W celu otrzymania wszelkich danych od firmy Allegro wynagane by≥o, abym podpisa≥ umowÍ, w ktÛrej zobowiπzujÍ siÍ do nieujawniania øadnych danych, ktÛre otrzyma≥em. Stπd opisy danych, na ktÛrych pracujÍ, zawarte w tej pracy nie wnikajπ w ich szczegÛ≥y i nieodbiegajπ od informacji publicznie dostÍpnych za poúrednictwem strony pod adresem \url{https://allegro.pl/artykuly}.

Aktualnie w rzeczonym dziale serwisu Allegro istnieje system rekomendacyjny, ktÛry opiera siÍ o wyszukiwanie podobnych artyku≥Ûw tekstowych za pomocπ silnika Elasticsearch\cite{elastic}. Metoda ta wykorzystuje s≥owa kluczowe przypisane do kaødego artyku≥u przez autora. W swojej pracy staram siÍ porÛwnaÊ wyniki dzia≥ania dotychczasowej metody z metodami semantycznej analizy tekstu, ktÛre potrafiπ wykryÊ podobieÒstwo pomiÍdzy artyku≥ami bazujπc jedynie na ich treúci, bez potrzeby do≥πczania øadnych metadanych. Pomyúlna prÛba zastosowania metod semantycznych pozwoli≥aby na dok≥adniejsze dopasowanie podobnych artyku≥Ûw w oparciu byÊ moøe o pewne ukryte cechy semantyczne nieosiπgalne dla silnika wyszukiwania tekstowego, jakim jest Elasticsearch. Bardziej szczegÛ≥owego opisu silnika Elasticsearch dokonujÍ w kolejnym rozdziale.

\section{Struktura pracy}
W rozdziale 2 wprowadzam do zagadnienia rekomendacji oraz dokonujÍ przeglπdu metod semantycznej analizy tekstu, ktÛre mogπ zostaÊ zastosowane w celu okreúlenia podobieÒstwa pomiÍdzy dokumentami tekstowymi.

NastÍpnie w rozdziale 3 dokonujÍ opisu konkretnego problemu, jakim jest generacja rekomendacji artyku≥Ûw tekstowych w serwisie Allegro. OpisujÍ dane otrzymane z serwisu oraz kolejne etapy ich wstÍpnego przetwarzania, aby nadawa≥y siÍ do zaaplikowania do nich wybranych metod.

Dalej, w rozdziale 4 opisujÍ stworzone i zastosowane pÛüniej metody ewaluacji wynikÛw.

NastÍpnie w rozdziale 5 dokonujÍ opisu testÛw: jakie metody i w jako sposÛb testujÍ.

W rozdziale 6 opisujÍ wyniki przeprowadzonych eksperymentÛw.

Ostatecznie w rozdziale 7 dokonujÍ podsumowania przeprowadzonych badaÒ i rozwaøam kierunki dalszych prac w tej dziedzinie.

Za≥πcznik A zawiera opis narzÍdzi programistycznych i bibliotek wykorzystanych przeze mnie podczas prowadzenia badaÒ.

\section{Uwagi}
W celu unikniÍcia nieporozumieÒ naleøy podkreúliÊ rÛønicÍ pomiÍdzy znaczeniami s≥owa ,,artyku≥'', ktÛre moøe oznaczaÊ zarÛwno tekst publicystyczny, literacki lub naukowy jak i rzecz, ktÛra jest przedmiotem handlu.\cite{slownik} W niniejszej pracy skupiam siÍ na rekomendacjach artyku≥Ûw tekstowych, stπd uøywam pierwszego znaczenia (chyba, øe inne znaczenie jest wyraünie zaznaczone).

\chapter{Przeglπd wiedzy z zakresu tematyki pracy}
W swojej pracy dokonujÍ adaptacji metod przetwarzania jÍzyka naturalnego na potrzeby generowania rekomendacji artyku≥Ûw tekstowych w oparciu o ich treúÊ. W niniejszym rozdziale dokonujÍ przeglπdu znanych metod z obszaru tematyki pracy dyplomowej, skupiajπc siÍ szczegÛlnie na nowo powsta≥ych metodach wektorowej reprezentacji s≥Ûw, ktÛre cieszπ siÍ obecnie duøym zainteresowaniem úrodowisk naukowych oraz firm komercyjnych. 

DokonujÍ krÛtkiego wprowadzenia do zagadnienia generowania rekomendacji, ktÛrego g≥Íboka analiza nie jest konieczna z punktu widzenia niniejszej pracy. NastÍpnie wykonujÍ chronologiczny przeglπd metod ciπg≥ej reprezentacji s≥Ûw zaczynajπc od trywialnych metod zliczania s≥Ûw (bag-of-words, tf-idf), przechodzπc przez metody wykorzystujπce koncepcjÍ tematÛw (Latent Semantic Analysis, Latent Dirichlet Allocation) i koÒczπc na g≥oúnych ostatnio metodach osadzania s≥Ûw w przestrzeni wektorowej (Word2vec, GloVe, FastText). Przy zarysie historycznym opieram siÍ w duøej mierze na artykule\cite{aylien}.

%CzÍúÊ z nich (metoda tf-idf, bag-of-words, silnik Elasticsearch oparty na indeksie Lucene\cite{lucene}) jest od lat powszechnie wykorzystywana w zadaniu wyszukiwania tekstowego. Inne z kolei - korzystajπce z semantycznej analizy tekstu - nie sπ tak popularne z powodu swojej nowoúci, bπdü trudnoúci w zaaplikowaniu. Daje to pole do badaÒ i ewentualnych usprawnieÒ istniejπcych systemÛw opierajπcych siÍ o klasyczne metody. Wybrane metody stosujÍ, zgodnie z tematem pracy, w zadaniu generowania rekomendacji, stπd przeglπd metod zaczynam w≥aúnie od wprowadzenia do tego zagadnienia.

\section{Systemy rekomendacji}

Systemy rekomendacji to narzÍdzia i techniki majπce na celu zasugerowaÊ uøytkownikowi przedmioty. Sugestie te odnoszπ siÍ do rÛønych procesÛw podejmowania decyzji takich jak np. ktÛre artyku≥y kupiÊ, jakiej muzyki s≥uchaÊ czy teø ktÛre wiadomoúci czytaÊ. ,,Przedmiot'' jest tutaj ogÛlnym pojÍciem oznaczajπcym coú, co system poleca uøytkownikowi. \cite{handbook} 

Przy wciπø wzrastajπcej iloúci danych uøytkownicy serwisÛw internetowych czÍsto nie sπ w stanie dotrzeÊ do informacji, ktÛra ich interesuje. Jest to pole do rozwoju zautomatyzowanych systemÛw rekomendacyjnych polecajπcych uøytkownikom treúci, ktÛre mogπ ich zainteresowaÊ. Dzia≥alnoúÊ tekiego systemu daje zysk zarÛwno uøytkownikowi, pozwalajπc mu dotrzeÊ do informacji, ktÛrej mÛg≥by samodzielnie nie odszukaÊ, albo wrÍcz nie wiedzieÊ, iø taka informacja istnieje, jak i w≥aúcicielowi serwisu internetowego, ktÛremu zaleøy, by przyciπgnπÊ do siebie uøytkownikÛw, aby ci w jak najwiÍkszym stopniu korzystali z ich us≥ug.

Sposoby dzia≥ania systemÛw rekomendacji moøna podzieliÊ na rÛøne warianty, spoúrÛd ktÛrych wyodrÍbniÊ moøna dwa najszerzej uøywane. Sπ to: filtrowanie kolaboratywne (collaborative filtering) i filtrowanie oparte na treúci (content-based filtering).

\subsection{Filtrowanie kolaboratywne (collaborative filtering)}
Technika ta opiera siÍ na spostrzeøeniu, iø uøytkownicy o podobnych preferencjach zachowujπ siÍ podobnie. Stπd jeøeli uøytkownik zachowuje siÍ podobnie do zaobserwowanej wczeúniej grupy uøytkownikÛw, moøna przewidzieÊ jego preferencje na podstawie zachowaÒ Ûw grupy. Istotnπ zaletπ tej metody jest fakt, iø nie zaleøy ona od dziedziny, w ktÛrej ulokowany jest system rekomendacji (w przeciwieÒstwie do rekomendacji opartych na treúci), a jedynie od zachowaÒ uøytkownikÛw.
\subsection{Filtrowanie oparte na treúci (content-based filtering)}
W technice tej przedmioty polecane uøytkownikowi zaleøπ od innych przedmiotÛw, na temat ktÛrych stwierdzono, øe uøytkownik siÍ nimi interesuje. Mogπ siÍ one opieraÊ np. na podobieÒstwie przedmiotÛw: jeøeli uøytkownik ,,lubi'' przedmiot A, ktÛry jest podobny do przedmiotu ,,B'' to moøna spodziewaÊ siÍ, øe rÛwnieø przedmiot B zainteresuje uøytkownika. Technika ta jest mocno zaleøna od dziedziny rekomendowanych przedmiotÛw, gdyø wymaga wprowadzenia pewnej miary podobieÒstwa miÍdzy nimi. Stπd jest trudniejsza do zastosowania, ale daje teø moøliwoúci nieosiπgalne dla filtrowania kolaboratywnego.

Celem niniejszej pracy jest zbadanie metod sugerujπcych uøytkownikowi artyku≥y podobne do aktualnie odwiedzanego, co wprost wiπøe siÍ z metodami uøywanymi w technice filtrowania opartego na treúci.

\section{Silnik Elasticsearch}

Obecnie wykorzystywana przez Allegro metoda generowania rekomendacji artyku≥Ûw opiera siÍ o zapytanie do us≥ugi Elasticsearch wykorzystujπce s≥owa kluczowe do≥πczone do artyku≥Ûw. Elasticsearch jest popularnym silnikiem wyszukiwania tekstu opartym o indeks Lucene\cite{lucene}. Dzia≥a w architekturze rozproszonej a komunikacja z nim nastÍpuje poprzez protokÛ≥ HTTP i format JSON.

%W niniejszej pracy wykonujÍ eksperymenty wykorzystujπc znane metody okreúlania podobieÒstw pomiÍdzy dokumentami, ktÛre adaptujÍ do zbioru dokumentÛw, ktÛre otrzyma≥em od serwisu Allegro.
%
%W obszarze, ktÛrym zajmuje siÍ niniejsza praca, bezpoúrednim celem rekomendacji jest, aby uøytkownik odwiedza≥ kolejne podstrony serwisu, co wprost zwiÍksza szansÍ na dokonanie przez niego transakcji.
%

%
%Metoda ta ogranicza siÍ jednak jedynie do wyszukiwania tekstowego pomijajπc zagadnienia semantyczne. Znaczy to, øe jeøeli dwa teksty opisujπ ten sam temat, ale uøywajπ to tego rÛønych s≥Ûw, np. synonimÛw, to systemowi opartemu jedynie o wyszukiwanie tekstowe nie uda siÍ stwierdziÊ podobieÒstwa miÍdzy tymi tekstami, mimo, iø takowe istnieje.
%
%Stπd w mojej pracy postanowi≥em wykonaÊ eksperymenty z metodami uøywajπcymi semantycznej analizy tekstu, aby oceniÊ, czy dajπ one lepsze rezultaty od obecnie stosowanej metody.
%
%W niniejszej pracy skupiam siÍ g≥Ûwnie na podejúciu word2vec z racji tego, iø powsta≥ niedawno.
%
%
%Dochodzenie nowych rekomendacji - nie jest tematem pracy
%


\section{Techniki przetwarzania jÍzyka naturalnego}

Oparcie rekomendacji jedynie na treúci artyku≥u wymaga zag≥Íbienia siÍ w tematykÍ analizy i przetwarzania jÍzyka naturalnego, wszak w≥aúnie w jÍzyku naturalnym, zrozumia≥ym dla cz≥owieka (polskim) pisane sπ owe artyku≥y. JÍzyk naturalny z powodu swojego niskiego stopnia sformalizowania nie jest niestety wprost zrozumia≥y dla maszyn. W zwiπzku z tym koniecznym staje siÍ tu uøycie technik przetwarzania jÍzyka naturalnego (natural language processing), ktÛre to pozwalajπ wyodrÍbniÊ z tekstu pewne cechy, na bazie ktÛrych maszyna obliczeniowa przy pomocy pewnych algorytmÛw jest w stanie okreúliÊ podobiÍÒstwo pomiÍdzy dokumentami. W poniøszych paragrafach dokonujÍ przeglπdu technik matematycznej reprezentacji dokumentÛw pisanych w jÍzyku naturalnym. Warto wspomnieÊ, iø dziedzina ta bardzo dynamicznie siÍ rozwija a czÍúÊ z opisywanych metod zosta≥o stworzonych na przestrzeni ostatnich kilku lat, czy wrÍcz miesiÍcy.

W celu formalizacji w dalszych opisach stosowanych metod stosujÍ nastÍpujπce oznaczenia:
\begin{itemize}
	\item Korpus $C$: zbiÛr dokumentÛw $d_i$,
	\item Dokument $d$: skoÒczony ciπg zdaÒ $s_i$,
	\item S≥owo $w$: skoÒczony ciπg znakÛw $c_i$,
	\item S≥ownik zbudowany na korpusie $C$: $V = \{w\ |\ \exists_{d \in C}\ w \in d\}$.
\end{itemize}

\subsection{Bag-of-words}
Bag-of-words (worek s≥Ûw)\cite{bow} jest jednπ z pierwszych koncepcji reprezentacji tekstu jako zbioru zawartych w nim s≥Ûw. Metoda nie zachowuje kolejnoúci s≥Ûw w tekúcie, lecz liczbÍ ich wystπpieÒ. TechnikÍ tÍ moøna opisaÊ jako przekszta≥cenie z korpusu w przestrzeÒ wektorÛw $ bow: C \to \mathbb{R}^n $ gdzie:\\
$C$: korpus\\
$m = |C|$: liczba dokumentÛw w korpusie $C$\\
$V$: s≥ownik zbudowany na $C$\\
$n = |V|$: liczba s≥Ûw w $V$\\
$v_i \in \mathbb{R}^n$, gdzie $i \in 1, 2, ..., n$ wektor reprezentujπcy dokument $d_i \in C$\\
$v_{ij}$, gdzie $j \in 1, 2, ..., m$: liczba wystπpieÒ w dokumencie  $d_i \in C$ s≥owa $w_j \in V$\\

Technika ta jest stosunkowo prosta, lecz jej wadπ jest traktowanie kaødego s≥owa z jednakowπ wagπ. Pewne s≥owa (np. ,,i'', ,,lub'', ,,o'') wystÍpujπ bardzo czÍsto, lecz ich wk≥ad w znaczenie ca≥ego dokumentu jest marginalny. Stπd powsta≥y bardziej zaawansowane techniki uwzglÍdniajπce istotnoúÊ s≥Ûw dla znaczenia ca≥ego dokumentu. Mimo to metoda BOW jest czÍsto wykorzystywana w bardziej zawansowanych technikach NLP.
\subsection{Term frequency - inverted document frequency}
%TODO Salton and McGill, 1983
TF-IDF (waøenie czÍstoúciπ termÛw - odwrotna czÍstoúÊ w dokumentach) jest metodπ reprezentacji tekstu jako zbioru s≥Ûw przy jednoczesnym uwzglÍdnieniu wagi s≥Ûw, ktÛra zaleøy od czÍstoúci wystÍpowania s≥owa w korpusie. Oznaczenia formalne takie same tak w przypdku BOW.
$v_{ij} = tfidf_{ij} = tf_{ij} * idf_i$, gdzie:\\
$tf_{ij} = \frac{n_{ij}}{\sum\limits_{k}n_{kj}}$, ,,term frequency'' to liczba wystπpieÒ s≥owa $w_i$ w dokumencie $d_j$ podzielona przez liczbÍ s≥Ûw dokumentu $d_j$,\\
$idf_i = log\frac{|D|}{|{d:w_i \in d}|}$, ,,inversed document frequecy'' to liczba dokumentÛw w korpusie podzielona przez liczbÍ dokumentÛw zawierajπcych przynajmniej jedno wystπpienie s≥owa $w_i$,

Dokumenty reprezentowane sπ tu jako wektory, sk≥adajπce siÍ z wag s≥Ûw wystÍpujπcych w kaødym z nich. TF-IDF przechowuje informacjÍ o czÍstotliwoúci wystÍpowania s≥Ûw biorπc przy tym pod uwagÍ istotnoúÊ znaczenia s≥owa lokalnego w stosunku do jego znaczenia w kontekúcie ca≥ego zbioru dokumentÛw. W tej technice s≥owa wystÍpujπce rzadko sπ premiowane wzglÍdem s≥Ûw pospolitych.

%In the popular tf-idf scheme (Salton and McGill, 1983), a basic vocabulary of ìwordsî or ìtermsî is chosen, and, for each document in the corpus, a count is formed of the number of occurrences of each word. After suitable normalization, this term frequency count is compared to an inverse document frequency count, which measures the number of occurrences of a word in the entire corpus (generally on a log scale, and again suitably normalized). The end result is a term-by-document matrix X whose columns contain the tf-idf values for each of the documents in the corpus. Thus the tf-idf scheme reduces documents of arbitrary length to fixed-length lists of numbers. While the tf-idf reduction has some appealing featuresónotably in its basic identification of sets of words that are discriminative for documents in the collectionóthe approach also provides a relatively small amount of reduction in description length and reveals little in the way of inter- or intradocument statistical structure. To address these shortcomings, IR researchers have proposed several other dimensionality reduction techniques, most notably latent semantic indexing (LSI) (Deerwester et al., 1990).

\subsection{Distributional semantics}
Kolejne, bardziej zaawansowane, omawiane tu metody opierajπ siÍ na tzw. ,,distributional hypothesis'' - hipotezie zak≥adajπcej, øe s≥owa wystÍpujπce w tym samym kontekúcie niosπ ze sobπ podobne znaczenie\cite{bow}\cite{firth}. Sprzyja to zastosowaniu metod algebry liniowej jako narzÍdzia obliczeniowego oraz sposobu reprezentacji tekstu. Podstawowe podejúcie polega na zgromadzeniu informacji o rozk≥adzie s≥Ûw w dokumentach w postaci wielowymiarowych wektorÛw a nastÍpnie wyodrÍbnieniu podobieÒstw pomiÍdzy tymi wektorami, ktÛre úwiadczy≥yby o pewnych powiπzaniach miÍdzy reprezentowanymi s≥owami. 

\subsection{Latent Semantic Analysis}
Alaliza rozk≥adu s≥Ûw w dokumentach tekstowych pozwala na wyodrÍbnienie podobieÒstw miÍdzy s≥owami pod kπtem: ich znaczenia (podobieÒstwo tematu s≥owa), ich osadzenia w stosunku do innych typÛw s≥Ûw czy teø ich struktury wewnÍtrznej. Dwie istotne metody: Latent Semantic Analysis\cite{lsa} oraz Latent Dirichlet Allocation\cite{lda} zak≥adajπ istnienie abstrakcyjnych niejawnych (latent) tematÛw, do ktÛrych moøna przydzieliÊ s≥owa wchodzπce w sk≥ad korpusu.

LSA (1990) \cite{lsa}, znane rÛwnieø jako Latent Semantic Indexing (LSI) dokonuje transformacji kaødego dokumentu w wektor d≥. $|V|$ posiadajπcy na $i$-tym miejscu wagÍ TF-IDF $i$-tego s≥owa ze s≥ownika. W ten sposÛb tworzona jest rzadka macierz: kolumny reprezentujπ dokumenty a wiersze reprezentujπ unikalne s≥owa. W celu identyfikacji istotnych cech tej macierzy dokonuje siÍ rozk≥adu wed≥ug wartoúci osobliwych (Singular Value Decomposition, SVD), ktÛry jest technikπ redukcji wymiarowoúci. Celem uøycia SVD jest redukcja liczby wierszy macierzy dla wydajniejszych dalszych obliczeÒ numerycznych oraz pozbycie siÍ szumÛw, utrzymujπc jednoczeúnie podobieÒstwa pomiÍdzy kolumnami. Ostatecznie uzyskuje siÍ macierz przynaleønoúci tematÛw do dokumentÛw, gdzie wiersze odpowiadajπce tematom moøna interpretowaÊ jako kombinacje pierwotnych wierszy-s≥Ûw o podobnym znaczeniu. Np. $\{(samochod), (ciagnik), (jezdnia)\} \to \{(1.3452 * samochod + 0.2828 * ciagnik + 0.3 * jezdnia)\}$. Wymiar uzyskiwanej macierzy jest ustalany za pomocπ hiperparamtru, ktÛry oznacza liczbÍ tematÛw. Uøywajπc uzyskanej macierzy, podobieÒstwo pomiÍdzy kolumnami-dokumentami obliczane jest wykorzystujπc odleg≥oúÊ kosinusowπ (opisanπ pÛüniej w tym rodziale). Metoda LSA ≥agodzi problem synonimÛw poprzez scalanie podobnych s≥Ûw w jeden temat. Niweluje rÛwnieø problem homonimÛw, w≥πczajπc je czÍúciowo w sk≥ad rÛønych tematÛw. Niemniej jednak poprzez arbitralne ustalanie hiperparametru odpowiedzialnego za liczbÍ tematÛw czÍúÊ semantycznie odrÍbnych tematÛw moøe zostaÊ wch≥oniÍta przez inne lub teø rozbicie na tematy moøe byÊ zbyt ,,drobne'' nie wykorzystujπc w pe≥ni semantycznych powiπzaÒ.

\subsubsection{Singular Value Decomposition}
%TODO czy opisywaÊ SVD
%(2003)
%This means that if we have an LSI representation of a collection of documents, a new document not in the collection can be ``folded in'' to this representation using Equation 244. This allows us to incrementally add documents to an LSI representation. Of course, such incremental addition fails to capture the co-occurrences of the newly added documents (and even ignores any new terms they contain). As such, the quality of the LSI representation will degrade as more documents are added and will eventually require a recomputation of the LSI representation.

\subsection{Latent Dirichlet Allocation}
%TODO opisaÊ metodÍ - jak bardzo szczegÛ≥owo?

%We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.

\subsection{Word embeddings}
Od 2013r., wraz z wprowadzeniem przez T. Mikolova metody Word2vec\cite{word2vec} nastπpi≥ gwa≥towny rozwÛj i niewπtpliwy sukces metod ,,word embeddings''. Okreúlenie ,,word embeddings'' oznacza osadzanie s≥Ûw w przestrzei wektorowej przy pomocy uczenia nienadzorowanego i zosta≥o po raz pierwszy uøyte 2003r. w pracy Y. Bengio\cite{bengio}, gdzie wektory s≥Ûw generowane sπ przez g≥Íbokπ sieÊ neuronowπ. OgÛ≥ technik zaliczanych obecnie do ,,word embeddings'' cechuje siÍ usi≥owaniem reprezentacji s≥Ûw wraz z zaleønoúciami pomiÍdzy nimi w postaci wektorÛw o stosunkowo niskiej wymiarowoúci. Dzieje siÍ to w opozycji do wczeúniejszych podejúÊ podobnych do Bag of words - produkujπcego ogromne, rzadkie wektory, ktÛrych wymiary rÛwnajπ siÍ rozmiarowi s≥ownika, o ktÛry oparty jest model (rzÍdu setek tysiÍcy). Waønπ w≥asnoúciπ metod osadzania s≥Ûw jest zachowanie przez wektory semantycznych i syntaktycznych w≥aúciwoúci s≥Ûw, co pozwala wykonywaÊ na nich operacje artymetyczne na wektorach odwzorowujπce cechy tychøe s≥Ûw np. $vector("king")-vector("man")+vector("woman") \approx vector("queen")$ 
%TODO obrazek z wektorami w przestrzeni
Stosowane obecnie podejúcia generowania wektorowych reprezentacji s≥Ûw moøna podzieliÊ na dwa typy:
\begin{enumerate}
	\item Modele predykcyjne: uczπ siÍ wektorowych reprezentacji s≥Ûw poprzez zmniejszenie b≥Ídu predykcji s≥Ûw naleøπcych do lokalnego kontekstu s≥owa $i$. Poniøej opisujÍ sztandarowy przyk≥ad takiego modelu - word2vec, gdzie sposobem na optymalizacjÍ funkcji celu jest zastosowanie p≥ytkiej sieci neuronowej typu feed-forward optymalizowanej za pomocπ metody stochastic gradient descent.
	\item Metody oparte o zliczanie: generujπ wektory s≥Ûw poprzez redukcjÍ wymiarowoúci w globalnej macierzy wspÛ≥wystπpieÒ s≥Ûw.
	Jako pierwszy etap konstruujπ one ogromnπ (wymiar rÛwna siÍ liczbie s≥Ûw w s≥owniku korpusu) macierz, ktÛra (podobnie, jak  metodzie LSI) nastÍpnie ulega faktoryzacji, aby uzyskaÊ macierz o mniejszym wymiarze, lecz nadal zachowujπcπ powiπzania pomiÍdzy s≥owami. Przyk≥adem jest tu opisana poniøej metoda Global Vectors - GloVe.
\end{enumerate}

Jednπ z szerokiego wachlarza moøliwoúci, jakie dajπ tego typu techniki jest okreúlanie podobieÒstwa pomiÍdzy ca≥ymi dokumentami, wykorzystujπc dodatkowe metody pozwalajπce przenieúÊ zaleønoúci miÍdzy poszczegÛlnymi s≥owami dokumentu na zaleønoúci miÍdzy ca≥ymi zbiorami s≥Ûw, co jest istotne z punktu widzenia tematu niniejszej pracy. Dwie z nich: metodÍ centroidu oraz Word Mover's Distance opisujÍ pÛüniej w tym rozdziale.

\subsection{Podejúcia deep learningowe}

Wspomniane podejúcie Bengio oparte jest o sieÊ neuronowπ typu feed-forward o jednej warstwie ukrytej zgodnie z architekturπ z poniøszego rysunku.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{img/bengio_language_model.png}
	\caption{Neuronowy model jÍzyka. èrÛd≥o: \cite{bengio}.}
\end{figure}

Celem dzia≥ania sieci jest maksymalizacja funkcji celu $J_\theta = \frac{1}{T}\sum\limits_{t=1}^T\ \text{log} \space f(w_t , w_{t-1} , \cdots , w_{t-n+1})$, gdzie $f(w_t , w_{t-1} , \cdots , w_{t-n+1})$ odpowiada prawdopodobieÒstwu $p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1})$ wystπpienia s≥owa $w_t$ bezpoúrednio po sekwencji s≥Ûw $w_{t-1} , \cdots , w_{t-n+1}$.


%In the neural network community, Collobert and Weston (2008) proposed to learn word embeddings using a feedforward ?These authors contributed equally. neural network, by predicting a word based on the two words on the left and two words on the right.
%TODO Collobert Weston
Wadπ obu opisanych rozwiπzaÒ siÍ wydajnoúÊ nauki na tyle niska, øe wykorzystanie modelu okazuje siÍ niemoøliwe w celach komercyjnych.
%Jednak istotnπ wadπ tych rozwiπzaÒ by≥a niska wydajnoúÊ ich nauki, co jest szczegÛlnie odczuwalne w przypadku ogromnych zbiorÛw danych wykorzystywanych w úrodowiskach produkcyjnych.
Rozwiπzaniem tego problemu wydajπ siÍ byÊ nowe metody wektorowej reprezentacji s≥Ûw powsta≥e na przestrzeni ostatnich lat. W odrÛønieniu do metod deep learningowych opierajπ siÍ one o metody szybkiej nauki, np. o p≥ytkie sieci neuronowe, ktÛre uczπ siÍ na tyle krÛtko, øe sprawdzajπ siÍ one w zastosowaniach komercyjnych


\subsection{Word2vec}%TODO przepisaÊ rozdzia≥ tak, by by≥ spÛjny z poprzednimi

Word2vec jest stosunkowo nowπ (2013r.) predykcyjnπ metodπ osadzania s≥Ûw w przestrzeni wektorowej, opisanπ w \cite{word2vec}. 

Autorzy metody proponujπ p≥ytkπ, dwuwarstwowπ sieÊ neuronowπ, ktÛra ma za zadanie odtworzyÊ kontekst danego s≥owa i na tej podstawie dokonaÊ reprezentacji s≥owa jako wektora liczb rzeczywistych. Jako wejúcie metoda otrzymuje s≥owa z korpusu, wyjúciem metody sπ natomiast wektory z pewnej N wymiarowej przestrzeni odpowiadajπce s≥owom wejúciowym. Uøyta tu sieÊ neuronowa sk≥ada siÍ z warstw: wejúciowej, jednej warstwy ukrytej i warstwy wyjúciowej. WyrÛønia siÍ dwie architektury sieci:
\begin{itemize}
	\item skip-gram: na podstawie s≥owa sieÊ dokonuje predykcji $N$ sπsiednich s≥Ûw. Zadaniem sieci neuronowej jest wtedy optymalizacja funkcji celu postaci $J_\theta = \frac{1}{T}\sum\limits_{t=1}^T\ \text{log} \space p(w_t \: | \: w_{t-n} , \cdots , w_{t-1}, w_{t+1}, \cdots , w_{t+n})$.
	\item CBOW (continuous bag of words): na postawie okna $N$ sπsiednich s≥Ûw sieÊ przewiduje s≥owo, ktÛrego z najwiÍkszym prawdopodobieÒstwem te $N$ s≥Ûw jest sπsiedztwem. W tym modelu funkcja celu przyjmuje postaÊ $J_\theta = \frac{1}{T}\sum\limits_{t=1}^T\ \sum\limits_{-n \leq j \leq n , \neq 0} \text{log} \space p(w_{t+j} \: | \: w_t)$.
\end{itemize}
%TODO opisaÊ literki w funkcji celu
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{img/skipgram_cbow.png}
	\caption{Schemat sieci wykorzystujπcej podejúcie skip-gram i CBOW. èrÛd≥o: \cite{word2vec}.}
	%\label{fig:warstwy}
\end{figure}

Wady i zalety obu podejúÊ sπ wymienione w \cite{google_word2vec}. W celu szczegÛ≥owego opisu metody Word2vec wprowadzam pojÍcie funkcji softmax.

Softmax jest generalizacjπ funkcji logistycznej, zamieniajπcπ $K$-wymiarowy wektor $z$ dowolnych liczb rzeczywistych na $K$-wymiarowy wektor liczb rzeczywistych z zakresu $(0,1]$, ktÛre sumujπ siÍ do $1$\cite{softmax}. Funkcja wyraøa siÍ wzorem $\sigma (\mathbf {z} )_{j}={\frac {e^{z_{j}}}{\sum _{k=1}^{K}e^{z_{k}}}}\ dla\ j=1,\ ...,K$. Wyjúcie funkcji moøna traktowaÊ jako pewien rozk≥ad prawdopodobieÒstwa.

Uøywajπc tej stosunkowo prostej architektury moøna wykonaÊ proces nauki uøywajπc milionÛw s≥Ûw, ktÛrych powiπzania miÍdzy sobπ zostanπ zachowane w systemie wag sieci neuronowej.

W metodzie Word2vec nauka polega na trenowaniu sieci neuronowej. Jednakøe w odrÛønieniu od innych metod wykorzystujπcych sieci neuronowe, Word2vec nie uøywa pÛüniej wytrenowanej sieci jako takiej, a jedynie otrzymanych w wyniku nauki wag warstwy ukrytej sieci, ktÛre faktycznie sπ wynikowymi wektorami s≥ow.

W dalszym opisie metody szczegÛ≥owo skupiam siÍ na podejúciu CBOW, lecz podejúcie skip-gram wyglπda analogicznie.

SieÊ neuronowa bÍdπca wynikiem nauki przyjmuje na wejúciu wektor binarny d≥ugoúci odpowiadajπcej liczbie s≥Ûw w s≥owniku V zbudowanym na korpusie treningowym. Wektor ten wype≥niony jest wartoúciami 0 oraz jednπ wartoúciπ 1 na i-tej pozycji. Taki wektor odpowiada i-temu s≥owu ze s≥ownika V. Wejúciem sieci sπ kolejne s≥owa z korpusu w tej w≥πúnie reprezentacji. Wyjúciem sieci jest wektor tej samej d≥ugoúci o wartoúciach rzeczywistych z zakresu [0,1], w ktÛrym wartoúÊ na i-tej pozycji odpowiada prawdopodobieÒstwu, øe i-te s≥owo ze s≥ownika znajduje siÍ w sπsiedztwie s≥owa wejúciowego. Za ,,sπsiedztwo'' wielkoúci x naleøy tu rozumieÊ zbiÛr z≥oøony z x s≥Ûw wystÍpujπcych przed danym s≥owem w korpusie i x s≥ow po≥oøonych za danym s≥owem. WartoúÊ x moøe byÊ tu ograniczona przez poczπtek/koniec zdania, ktÛre ograniczajπ kontekst danego s≥owa.

Jako efekt naleøy siÍ spodziewaÊ, øe dla s≥owa wejúciowego ,,Brytania'' otrzymamy na wyjúciu wysokπ wartoúÊ prawdopodobieÒstwa dla s≥owa ,,Wielka'', a niskπ np. dla s≥owa ,,skoroszyt''.

Jednym z parametrÛw metody Word2vec jest wymiarowoúÊ przestrzeni, w ktÛrej znajdujπ siÍ otrzymane wektory odpowiadajπce s≥owom z korpusu. Liczba ta ma swoje ürÛd≥o z wielkoúci warstwy ukrytej sieci neuronowej. Wagi warstwy ukrytej moøna interpretowaÊ jako macierz $M$x$N$, gdzie $M$ to liczba s≥Ûw s≥ownika $V$ - wielkoúÊ wektowa wejúciowego, a $N$ to liczba neuronÛw w warstwie ukrytej. Po przeprowadzeniu nauki $i$-ty wiersz tej macierzy odpowiada wektorowi d≥ugoúci $N$, ktÛry reprezentuje $i$-te s≥owo ze s≥ownika $V$.

W sieci nie jest uøywa funkcja akywacji, ale prawdopodobieÒstwa na wyjúciu sπ efektem dzi≥ania funkcji softmax. Funkcja ta ma za zadanie sprowadziÊ wyjúciowe wartoúci warstwy ukrytej do postaci rozk≥adu prawdopodobieÒstwa. 

Isotnπ zaletπ metody Word2vec jest fakt, iø pozwala ona oceniÊ ,,odleg≥oúÊ'' pomiÍdzy dwoma dokumentami nawet, jeøeli nie posiadajπ one wspÛlnych s≥Ûw.

%TODO OpisaÊ co to jest n-gram
\subsection{FastText}

%Most of these techniques represent each word of the vocabulary by a distinct vector, without parameters sharing. In particular, they ignore the internal structure of words, which is an important limitation for morphologically rich languages, such as Turkish or Finnish. These languages contain many words that occur rarely, making it difficult to learn good word-level representations. In this paper, we propose to learn representations for character n-grams, and represent words as the sum of the n-gram vectors. Our main contribution is to introduce an extension of the continuous skip-gram model (Mikolov et al., 2013b), which takes into account subword information. We evaluate this model on five different languages, with various degree of morphology, showing the benefit of our approach.

Rozszerzeniem koncepcji word2vec jest opracowana w 2016 przez Facebook AI Research metoda fastText\cite{fast_text}. G≥Ûwnπ innowacjπ tej metody jest wziÍcie pod uwagÍ wewnÍtrznej struktury s≥Ûw. Jest to szczegÛlnie obiecujπce w przypadku bogatych morfologicznie jÍzykÛw, np. jÍzyka polskiego.

Metoda fastText w duøej mierze opiera siÍ na zasadzie dzia≥ania metody word2vec. Najistotniejszπ rÛønicπ jest sposÛb poziom szczegÛ≥owoúci analizy s≥Ûw. Word2vec traktuje s≥owo jako niepodzielnπ jednostkÍ, ignorujπc wewnÍtrznπ strukturÍ jego znakÛw. Zgodnie z opisem z powyøszego paragrafu kaøde s≥owo przygotowywane jest do wejúcia do sieci neuronowej w postaci unikalnego wektora wype≥nionego zerami z wartoúciπ '1' na jednej pozycji (,,one-hot-vector''). FastText natomiast analizuje kaøde s≥owo pod kπtem struktury jego znakÛw. Wykorzystuje do tego rozbicie s≥owa na pods≥owa - ciπgi znakÛw o okreúlonej d≥ugoúci ,,character n-grams''. 

S≥owo reprezentowane jest jako suma wektorowych reprezentacji jego n-gramÛw.


%In this section, we thus propose a different scoring function s, in order to take into account this information. Given a word w, let us denote by Gw ? {1, . . . , G} the set of n-grams appearing in w. We associate a vector representation zg to each n-gram g. We represent a word by the sum of the vector representations of its n-grams. We thus obtain the scoring function s(w, c) = X g?Gw z ? g vc. We always include the word w in the set of its ngrams, to also learn a vector representation for each word. The set of n-grams is thus a superset of the vocabulary. It should be noted that different vectors are assigned to a word and a n-gram sharing the same sequence of characters. For example, the word as and the bigram as, appearing in the word paste, will be assigned to different vectors. This simple model allows sharing the representations across words, thus allowing to learn reliable representation for rare words. Dictionary of n-grams. The presented model is simple and leaves room for design choices in the definition of Gw. In this paper, we adopt a very simple scheme: we keep all the n-grams with a length greater or equal than 3 and smaller or equal than 6. Different sets of n-grams could be used, for example prefixes and suffixes. We also add a special character for the beginning and the end of the word, thus allowing to distinguish prefixes and suffixes. In order to bound the memory requirements of our model, we use a hashing function that maps n-grams to integers in 1 to K. In the following, we use K equal to 2 millions. In the end, a word is represented by its index in the word dictionary and the value of the hashes of its n-grams. To improve the efficiency of our model, we do not use n-grams to represent the P most frequent words in the vocabulary. There is a trade-off in the choice of P, as smaller values imply higher computational cost but better performance. When P = W, our model is the skip-gram model of Mikolov et al. (2013b).


Jak pokazuje badanie\cite{fast_text_word2vec} metoda ta sprawdza siÍ lepiej od word2vec w wykrywaniu syntaktycznych podobieÒstw pomiÍdzy s≥owami. 

\subsection{GloVe}

%TODO glove stara siÍ osiπgnπÊ to, co w word2vec wydaje siÍ byÊ jedynie efektem ubocznym. Zmienia w tym celu funkcjÍ celu.... http://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/

GloVe\cite{glove} (GLObal VEctors) jest kolejnπ wartπ uwagi metodπ word embedding powsta≥π na przestrzeni ostatnich lat. Algorytm GloVe rÛøni siÍ od word2vec w sposobie uzyskania wektorowej reprezentacji s≥Ûw. Word2vec jest modelem predykcyjnym, natomiast trening w GloVe opiera siÍ na globalnej macierzy wspÛ≥wystπpieÒ s≥Ûw.

Algorytm GloVe sk≥ada siÍ z nastÍpujπcych krokÛw\cite{glove_cran}:
\begin{enumerate}
	\item Zgromadü wspÛ≥wystπpienia s≥Ûw w formie macierzy $X$. Kaødy element $X_{ij}$ takiej macierzy reprezentuje jak czÍsto s≥owo $i$ wystÍpuje w pobliøu s≥owa $j$. Zazwyczaj macierz buduje siÍ poprzez skanowanie bazowego korpusu oknem o ustalonej szerokoúci, w obrÍbie ktÛrego centralne s≥owo leøy w kontekúcie s≥Ûw je otaczajπcych. Dodatkowo moøna tu wprowadziÊ wagi dla s≥Ûw malejπce wraz ze wzrostem dystansu od s≥owa centralnego.
	\item Zdefiniuj ograniczenie dla kaødej pary s≥Ûw: $w_i^Tw_j + b_i + b_j = log(X_{ij})$, gdzie $w_i$ oznacza wektor g≥Ûwnego s≥owa, $w_j$ s≥owa leøπcego w pobliøu $i$, $b_i$ i $b_j$ to skalary.
	\item Zdefiniuj funkcjÍ kosztu $J = \sum_{i=1}^V \sum_{j=1}^V \; f(X_{ij}) ( w_i^T w_j + b_i + b_j - \log X_{ij})^2$, gdzie f jest funkcjπ waøπcπ, ktÛra pomaga zapobiec uczeniu tylko na podstawie najbardziej popularnych par s≥Ûw. Autorzy proponujπ funkcjÍ postaci:
	$f(X_{ij}) = 
	\begin{cases}
	(\frac{X_{ij}}{x_{max}})^\alpha & \text{if } X_{ij} < XMAX \\
	1 & \text{otherwise}
	\end{cases}$
\end{enumerate}

\subsection{Odleg≥oúÊ miÍdzy dokumentami}
W celu wykorzystania omÛwionych metod osadzania s≥Ûw naleøy wybraÊ metodÍ obliczania odleg≥oúci miÍdzy ca≥ymi dokumentami, ktÛrych s≥owa potrafimy reprezentowaÊ jako wektory. Zak≥adamy, øe jeøeli dystans pomiÍdzy dokumentami jest ma≥y, to ich tematyka jest podobna.
\subsubsection{Centroid}
Najprostszπ i najbardziej intuicyjnπ metodπ obliczenia odleg≥oúci pomiÍdzy wektorowπ reprezentacjπ dokumentÛw jest wykonanie dwÛch prostych krokÛw:
\begin{enumerate}
	\item Uúrednienie wektorÛw wchodzπcych w sk≥ad kaødego z dokumentÛw. Powsta≥y w ten sposÛb wektor jest centroidem reprezentujπcym dokument w przestrzeni wektorowej.
	\item Obliczenie dystansu miÍdzy wektorami. Powszechnie przyjÍtπ praktykπ jest stosowanie tzw. odleg≥oúci kosinusowej - znormalizowanego iloczynu skalarnego wektorÛw $A$ i $B$. Jest to kosinus kπta pomiÍdzy dwoma wektorami reprezentujπcymi dokumenty. Zaletπ tej metody jest natychmiastowa normalizacja wyniku do zakresu $(0, 1)$. Odleg≥oúÊ $sim={\mathbf {A} \cdot \mathbf {B}  \over \|\mathbf {A} \|_{2}\|\mathbf {B} \|_{2}}={\frac {\sum \limits _{i=1}^{n}{A_{i}B_{i}}}{{\sqrt {\sum \limits _{i=1}^{n}{A_{i}^{2}}}}{\sqrt {\sum \limits _{i=1}^{n}{B_{i}^{2}}}}}}$,
	gdzie $A_i$ i $B_i$ sπ sk≥adowymi wektorÛw odpowiednio $A$ i $B$
\end{enumerate}
Wadπ opisanej powyøej metody jest utrata potencjalnie uøytecznych zaleønoúci pomiÍdzy poszczegÛlnymi wektorami wchodzπcymi w sk≥ad dokumentu.

W kontrze to tego prezentujÍ metodÍ obliczania dystansu miÍdzy dokumentami uwzglÍdniajπcπ rozk≥ad wektorÛw wewnπtz dokumentu.

\subsubsection{Word Mover's Distance}
Word Mover's Distance\cite{wmd} to rozwiπzanie zwracajπce odleg≥oúÊ miÍdzy dokumentami tekstowymi. W tym celu adaptuje algorytm Earth Mover's Distance\cite{emd} oraz wektorowπ reprezentacjÍ s≥Ûw dokumentu. WMD mierzy odleg≥oúÊ miÍdzy dokumentami jako minimalny dystans jaki wektory s≥Ûw pierwszego dokumentu muszπ ,,pokonaÊ'' aby osiπgnπÊ wartoúci wektorÛw z drugiego dokumentu.

EMD jest metodπ mierzenia odleg≥oúci pomiÍdzy dwoma rozk≥adami, ktÛra opiera siÍ na minimalnym koszcie, jaki musi zostaÊ poniesiony, aby dokonaÊ transformacji jednego rozk≥adu w drugi. Problem moøna sformalizowaÊ jako problem programowania liniowego, gdzie:
$P=\{f(p_1,w_{p_1})...(p_m,w_{p_m})\}$, $Q=\{f(q_1,w_{q_1})...(q_n,w_{q_n})\}$ sπ danymi rozk≥adami o $m$ (odpowiednio $n$) klastrach $p_i$ ($q_j$), a $w_{p_i}$ ($w_{q_j}$) jest masπ klastra. $D=[d_{ij}]$ jest macierzπ odleg≥oúci, w ktÛrej $d_{ij}$ reprezentuje odleg≥oúÊ pomiÍdzy klastrami $p_i$ i $q_j$. Celem jest znaleüÊ taki przep≥yw $F = [f_{ij}]$, gdzie $f_{ij}$ to przep≥yw pomiÍdzy $p_i$ i $q_j$, ktÛry minimalizuje ca≥oúciowy koszt $Work(P, Q, F) = \sum_{i=1}^{m}\sum_{n}^{j=1}d_{ij}f_{ij}$ przy odpowiednich ogramiczeniach\cite{emd}.
EMD jest to dobrze zbadanym problemem transportowym\cite{emd}, dla ktÛrego powsta≥y efektywne metody rozwiπzania\cite{emd_method}. 

PrzypuúÊmy, øe dziÍki metodzie Word2vec dla s≥ownika $V$ o $n$ s≥owach otrzymujemy macierz $X \in \mathbb{R}^{d \times n}$. $i$-ta kolumna tej macierzy reprezentuje $i$-te s≥owo ze s≥ownika $V$. Odleg≥oúci pomiÍdzy wektorami reprezentujπcymi semantycznie zbliøone s≥owa sπ relatywnie mniejsze od odleg≥oúci dla s≥Ûw niezwiπzanych ze sobπ. Celem WMD jest zawrzeÊ semantyczne podobieÒstwo pomiÍdzy poszczegÛlnymi parami s≥Ûw w dystans pomiÍdzy ca≥ymi dokumentami. Aby to osiπgnπÊ metoda traktuje dokument jako rozk≥ad, ktÛrego $i$-tym elementem jest liczba wystπpieÒ $i$-tego s≥owa w tym dokumencie, a nastÍpnie stosuje metodÍ EMD do obliczenia dystansu miÍdzy tymi rozk≥adami. Macierz odleg≥oúci $D$ uøywana w metodzie EMD jest zbudowana na bazie odleg≥oúci miÍdzy wektorami Word2vec reprezentujπcymi s≥owa dokumentÛw. $d_{ij} = ||x_i-x_j||$, gdzie $i$ i $j$ to indeksy s≥Ûw ze s≥ownika $V$ a $x_{ij}$ to element macierzy $X$\cite{wmd}. Autorzy metody okreúlajπ z≥oøonoúÊ metody jako $O(p^3\log p)$, gdzie $p$ to wielkoúÊ s≥ownika $V$.


\chapter{Dane}%TODO przerobiÊ ca≥y rozdzia≥
%opis specyfiki danych
Dane, na ktÛrych testowane by≥y opisywane w niniejszej pracy metody otrzyma≥em dziÍki øyczliwoúci serwisu Allegro. Jednak, by dane te otrzymaÊ, zobowiπzany zosta≥em po podpisania umowy o poufnoúci. Stπd, w niniejszej pracy brak jakichkolwiek przyk≥adÛw danych, a jedynie opisy metod uøytych do ich przetwarzania i generowania rekomendacji.

\section{Opis danych}

Otrzymane dane to baza ok. 20000 artyku≥Ûw tekstowych w formacie JSON. Sπ to te same artyku≥y, ktÛre sπ dostÍpne dla uøytkownikÛw poprzez serwis internetowy (stan na styczeÒ 2017). Pojedynczy rekord danych sk≥ada siÍ z g≥Ûwnej treúci artyku≥u oraz z metadanych, z ktÛrych za istotne z punktu widzenia tematu pracy uzna≥em pola: id, kategoria i s≥owa kluczowe.

\subsection{TreúÊ artyku≥u}

TreúÊ kaødego artyku≥u sk≥ada siÍ z trzech pÛl: ,,zawartoúÊ'', ,,nag≥Ûwek'' i ,,tytu≥''. årednia d≥ugoúÊ artyku≥u to 821 s≥Ûw, w tym nag≥Ûwek to jednozdaniowy wstÍp. åredniπ tπ estymujÍ na podstawie úredniej liczby znakÛw artyku≥u i úredniej d≥ugoúci s≥owa w jÍzyku polskim. Dok≥adne statystyki tekstu bÍdπ dostÍpne dopiero po wstÍpnym przetwarzaniu.

Wszystkie artyku≥y napisane sπ w jÍzyku polskim, w nielicznych przypadkach wykry≥em b≥Ídy, tzw. ,,literÛwki''. Jako, øe artyku≥y ze zbioru dotyczπ produktÛw sprzedawanych za poúrednictwem serwisu Allegro, w sk≥ad s≥ownika zbudowanego na ich bazie wchodzi wiele s≥Ûw specyficznych dla rÛønych branø. Sπ to m.in. nazwy modeli aparatÛw (np. ,,Sony Alpha 77 II''), samochodÛw, gier komputerowych, a takøe nazwy techniczne: ,,sprÍøarka'', ,,hipertoniczny'', ,,autofocus''.

Artyku≥y posiadajπ w swej treúci wiele znacznikÛw interpretowanych przez system, na podstawie ktÛrych wzbogacana jest warstwa wizualna strony internetowej zawierajπcej artyku≥, np. obrazki czy ≥πcza do ofert zwiπzanych z tematem artyku≥u.

SpÛjnoúÊ danych oceniam na wysokπ, tj. kaøde pole zawarte w strukturze dokumentu jest zawsze wype≥nione - brak jest wartoúci typu NULL.

\subsection{Kategoria}

Kaødy artyku≥ zosta≥ przez autora przydzielony do pewnej kategorii, ktÛra odpowiada tematyce artyku≥u, np. ,,Aparaty cyfrowe'' czy ,,Przyprawy i zio≥a''. W sk≥ad pola ,,kategoria'' wchodzi rÛwnieø lista kategorii nadrzÍdnych, a ca≥a hierarcha kategorii ma strukturÍ drzewiastπ. Np. kategoria nadrzÍdna dla kat. ,,Przyprawy i zio≥a'' to ,,Delikatesy'', a dla kat. ,,Delikatesy'' to  ,,Dom i zdrowie''. Kaødy artyku≥ naleøy do tylko jednej kategorii bÍdπcej dowolnym wÍz≥em w drzewie (nie tylko liúciem).

\subsection{S≥owa kluczowe}%TODO opisaÊ pole

Do kaødego artyku≥u do≥πczone sπ s≥owa kluczowe charakteryzujπce jego zawartoúÊ, np. ,,aparaty'', ,,aparaty cyfrowe'', ,,lustrzanki'', ,,sony''. Pole to jest wykorzystywane w dotychczasowym mechaniümie generowania rekomendacji - artyku≥y podobne do danego sπ wyszukiwane na podstawie jego s≥Ûw kluczowych.

\section{WstÍpne przetwarzanie danych}%TODO przeredagowaÊ

W celu zwiÍkszenia skutecznoúci metod analizy tekstu stosuje siÍ wstÍpne przetwarzanie danych. Ma ono na celu takie przygotowanie tekstu, aby zmaksymalizowaÊ jakoúÊ wynikÛw operujπcych na nim pÛüniej algorytmy. Techniki wstÍpnego przetwarzania tekstu nie wchodzπ w sk≥πd øadnego standardu - dobieram je indywidualnie do konkrtenego przypadku, zgodnie z intuicjπ.

Niøej opisujÍ kolejne kroki wstÍpnego przetwarzania tekstu, ktÛre wykonujÍ na posiadanym zbiorze artyku≥Ûw.
\begin{enumerate}
	\item Oczyszczanie tekstu ze zbÍdnych, wspomnianych wczeúniej znacznikÛw. Z punktu widzenia semantycznej analizy tekstu sπ one bezuøyteczne, czy wrÍcz szkodliwe (powodujπ pewne ,,zanieczyszczenie'' tekstu). Stπd usuwam je wykorzystujπc odpowiednio skonstruowane wyraøenia regularne (ich postaÊ jest szczegÛ≥em nieistotnym z punktu widzenia tematyki niniejszej pracy).
	\item UsuniÍcie ,,s≥Ûw stopu''(ang. stopwords) - na ogÛ≥ krÛtkich s≥Ûw nie wnoszπcych nic do znaczenia ca≥oúci artyku≥u. Sπ to np. ,,w'', ,,z'', ,,poniewaø''. Ich usuniÍcie zmniejsza liczbÍ s≥Ûw dokumentu skracajπc tym samym czas jego przetwarzania. Jako øe s≥owa te wystÍpujπ czÍsto, usuniÍcie ich daje moøliwoúÊ uwypuklenia znaczenia innych s≥Ûw majπcych wp≥yw na rzeczywiste znaczenie ca≥ego artyku≥u. ZbiÛr s≥Ûw stopu czerpiÍ z \cite{stopwords}.
	\item Sprowadzenie wszystkich s≥Ûw dokumentu do ma≥ych liter. Pomaga to ujednoliciÊ postaÊ czÍúci s≥Ûw o tym samym znaczeniu, wúrÛd ktÛrych jedno wystÍpuje na poczπtku zdania a inne w úrodku.
	\item Rozbicie s≥Ûw po≥πczonych myúlnikiem. Doúwiadczenie w pÛüniejszym etapie (tokenizacji) pokazuje, øe narzÍdzie jej dokonujπce nie radzi sobie z tego typu s≥owami (np. ,,bia≥o-czerwony'') i zostania je w niezmienionej postaci gramatycznej (np. ,,bia≥o-czerwonego''). Stπd koniecznoúÊ rÍcznego wykoania mechanizmu rozbijajπcego takie s≥owa do postaci kompatybilnej z tokenizerem.
	\item Tokenizacja. Jest to najistotniejszy element ca≥ego procesu. Polega na sprowadzaniu s≥Ûw o tym samym znaczeniu, a rÛønej formie gramatycznej do tej samej postaci. Sporym utrudnieniem jest tutaj stopieÒ skomplikowania jÍzyka polskiego oraz liczba wyjπtkÛw, jakπ ten jÍzyk posiada. Za przyk≥ad moøe pos≥uøyÊ s≥owo ,,mieÊ'', ktÛrego jedna z form to ,,ma'', kolejna to ,,miej''. Celem etapu jest sprowadzenie kaødego z tych wyrazÛw do formy podstawowej ,,mieÊ''. Do przeprowadzenia tej operacji stosujÍ narzÍdzie Morfologik\cite{morfologik}.
\end{enumerate}

Uøycie wymienionych technik nie jest jedynym standardem a wynikiem analizy przetwarzanych danych i techniki te zosta≥y dobrane dla tego konkretnego przypadku

\section{Opis danych po wstÍpnym przetwarzaniu}

Powyøsze kroki doprowadzajπ dane do stanu, w ktÛrym moøna zastosowaÊ techniki semantycznej analizy tekstu. S≥ownik zbudowany na wstÍpnie przetworzonym korpusie zawiera 98174 unikalnych s≥Ûw, oraz 7409145 wszystkich s≥Ûw (z powtÛrzeniami).

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{img/words_hist_log.png}
	\caption{Histogram liczby wystπpieÒ s≥Ûw w korpusie w skali logarytmicznej.}
	%\label{fig:warstwy}
\end{figure}

WiÍkszoúÊ artyku≥Ûw okaza≥a siÍ byÊ podobnej d≥ugoúci, úrednia d≥ugoúÊ artyku≥u to 370 s≥Ûw.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{img/articles_length_hist.png}
	\caption{Histogram d≥ugoúci artyku≥Ûw.}
	%\label{fig:warstwy}
\end{figure}

\chapter{Metody ewaluacji}
W celu porÛwnania stosowanych metod wyznaczania podobieÒstwa miÍdzy artyku≥ami konieczna jest formalizacja pewnych miar tego podobieÒstwa.

Ewaluacja rankingu, w ktÛrym trafnoúÊ wynikÛw zaleøy od oceny uøytkownikÛw jest zadaniem nietrywialnym. PodobieÒstwo artyku≥Ûw napisanych w jÍzyku naturalnym jest rzeczπ subiektywnπ. W sytuacji idealnej dysponowalibyúmy obiektywnπ miarπ podobieÒstwa pomiÍdzy parami N artyku≥Ûw (np. wyznaczonπ wczeúniej przez miarodajnπ grupÍ uøytkownikÛw), ktÛre to N artyku≥Ûw stanowi≥oby zbiÛr testowy. Uzyskanie takich danych wiπøe siÍ jednak z duøymi kosztami i leøy poza moimi moøliwoúciami.

Praktykπ umoøliwiajπcπ obiektywnπ ocenÍ, wykorzystywanπ w dzia≥ajπcych systemach sπ tzw. testy A/B polegajπce na podziale uøytkownikÛw na grupy i zaaplikowaniu kaødej grupie innego rozwiπzania. NastÍpnie mierzone sπ pewne wskaüniki wúrÛd kaødej grupy (w naszym przypadku np. liczba ,,klikniÍÊ'' prawdziwych uøytkownikÛw w artyku≥y rekomendowane) i spoúrÛd zgromadzonych wynikÛw wybierane jest rozwiπzanie najlepsze.

Z powodu braku moøliwoúci wykorzystania rzeczywistych uøytkownikÛw do ewaluacji rozwiπzaÒ jestem zmuszony wprowadziÊ w≥asne miary oparte na dostÍpnych danych. Naleøy tu zaznaczyÊ niedoskona≥oúÊ wprowadzanych miar, poniewaø kaøda z nich opiera siÍ na pewnyh za≥oøeniach, od ktÛrych prawdziwoúci zlaeøy jakoúÊ ca≥ej miary.

Dzia≥anie testowanych metod moøna sformalizwaÊ w postaci pewnej funkcji $S_n: C \to \{a_{i}\}_{i < n}$, gdzie $a_i \in C$, a $n$ to liczba elementÛw zwracanego ciπgu. Funkcja $S$ przyjmuje artyku≥ tekstowy (bπdü jego identyfikator) i zwraca skoÒczony ciπg artyk≥Ûw do niego podobnych zgodnie ze stopniem dopasowania (najlepsze na poczπtku). Celem dzia≥ania niøej opisanych miar jest kaødej parze postaci: wyjúcie-wejúcie funkcji $S$ reprezentujπcej testowanπ metodÍ przypisaÊ ocenÍ jakoúci zwrÛconego wyjúcia dla danego wejúcia. Oceny dla konkretnej metody, dla ustalonej prÛby artyku≥Ûw sπ nastÍpnie uúredniane.

Opisane poniøej miary 1 i 2 dokonujπ porÛwnania podobieÒstwa dla pary artyku≥Ûw. W celu rozszerzenia dzia≥ania tych miar do pary wejúcie-wyjúcie metody stosujÍ úredniπ waøonπ podobieÒstwa kolejnych elementÛw wyjúcia z wejúciem. Stosowane wagi: $\dfrac{1}{i}$ dla $i=1,\ ...,\ N$, gdzie $N$ to d≥ugoúÊ ciπgu wyjúciowego danej metody.


\section{Miara 1: Dystans oparty na metadanych}

Jak wspomnia≥em wczeúniej dane prÛcz treúci artyku≥Ûw zawierajπ rÛwnieø pewne metadane, a wúrÛd nich umoøliwiajπce tworzenie powiπzaÒ miÍdzy artyku≥ami. Skupiam siÍ tu na polach: ,,s≥owa kluczowe'' i ,,kategoria''.

\subsection{Kategorie} 

Pierwszπ zastosowanπ miarπ, pozwalajπcπ oceniÊ jakoúÊ dopasowania podobnych artyku≥Ûw jest ich odleg≥oúÊ we wczeúniej wspomnianym drzewie kategorii. Zak≥adam tu, øe im wiÍcej wspÛlnych przodkÛw w drzewie, tym bardziej podobe do siebie sπ artyku≥y reprezentowane przez wÍz≥y drzewa. Zaletπ miary jest fakt, iø przypisanie artyku≥u do kategorii zosta≥o wykonane przez autora, ktÛrego moøna okreúliÊ ekspertem w dziedzinie tematyki artyku≥u. Stπd przynaleønoúÊ artyku≥u do danej kategorii jest mocno uzasadniona. Kolejnπ zaletπ tej miary jest fakt, iø moøna jπ zastosowaÊ automatycznie - wiedza ekspercka jest juø zapisana w danych artyku≥Ûw. Naleøy zaznaczyÊ tu jednak, øe miara nie jest idealna - kaødy artyku≥ naleøy do tylko jednego liúcia drzewa kategorii. Stπd artyku≥ poruszajπcy zagadnienia z rÛønych obszarÛw, ktÛry moøna by przypisaÊ dwÛm stosunkowo odleg≥ym kategoriom $A$ i $B$, zostanie przypisany tylko do jednej kategorii, np. $A$. Miara pokaøe wtedy duøπ odleg≥oúÊ od artyku≥Ûw z kategorii $B$, co nie jest prawdπ.

Formalnie miarÍ moøna zapisaÊ jako: 
$d(a_1, a_2) = \dfrac{w(a_1,a_2)}{D}$, gdzie $d$ to dystans miÍdzy artyku≥ami $a_1$ i $a_2$, $w(x, y)$ to d≥ugoúÊ czÍúci wspÛlnej úcieøek od korzenia drzewa kategorii do wÍz≥Ûw reprezentujπcych artyku≥y $x$ i $y$, a $D$ to g≥ÍbokoúÊ ca≥ego drzewa (wprowadzone w celu normalizacji). Im wyøszy wynik, tym wiÍksze podobieÒstwo artyku≥Ûw.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{img/cat_tree_example_1.png}
	\caption{Drzewo kategorii dla przyk≥adu 1.}
	%\label{fig:warstwy}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{img/cat_tree_example_2.png}
	\caption{Drzewo kategorii dla przyk≥adu 2.}
	%\label{fig:warstwy}
\end{figure}
W powyøszych przyk≥adowych drzewach $w(X, Y) = 1$, $w(Y, Z) = 2$, $D=3$, stπd $d(X, Y) = \dfrac{1}{3}$, $d(X, Y) = \dfrac{2}{3}$. Miara wskazuje, øe artyku≥y $X$ i $Y$ sπ do siebie mniej podobne, niø artyku≥y $Y$ i $Z$.

\subsection{S≥owa kluczowe}
%TODO, opisaÊ nowπ miarÍ

\section{Miara 2: Ocena przez uøytkownikÛw offline}

Kolejnπ wypracowanπ miarπ jest subiektywna ocena ekspercka. W celu obiektywizacji oceny, ewaluacja powinna byÊ dokonana przez grupÍ osÛb operujπcycha na tych samych danych. Wadπ tej metody jest jej powolnoúÊ i potrzeba zaangaøowania dodatkowych osÛb dokonujπcych ewaluacji. Niemoøliwym wydaje siÍ przeprowadzenie badania dla wszystkich artyku≥Ûw, stπd konieczny jest wybÛr losowej prÛby artyku≥Ûw, ktÛre parami poddane zostanπ ocenie pod kπtem podobieÒstwa. Skala ocen to 1-10: 1, gdy artyku≥y nie sπ do siebie podobne, 10, gdy podobieÒstwo jest ca≥kowite.

\section{Miara 3: Historyczna aktywnoúÊ uøytkownikÛw serwisu}

Zbieranie a nastÍpnie przechowywanie informacji o aktywnoúci uøytkownika w ramach serwisu internetowego jest powszechnπ praktykπ. Proces ten pozwala na analizÍ zachowania uøytkownikÛw co moøe doprowadziÊ do wnioskÛw, jakie usprawnienia naleøy przedsiÍwziπÊ, aby spe≥niÊ cele biznesowe. Jednym z przyk≥adÛw aktywnoúci uøytkownika zapisywanej przez serwis Allego sπ klikniÍcia w linki znajdujπce siÍ na stronie internetowej. Informacja ta pozwala sporzπdziÊ jeszcze jednπ miarÍ jakoúci dopasowania podobnych do siebie artyku≥Ûw. PostaÊ danych, jakie uda≥o mi siÍ uzyskaÊ z serwisu to tabela o polach: adres strony, na ktÛrej nastπpi≥o klikniÍcie, adres strony, na ktÛrπ prowadzi link, data klikniÍcia.

Zaletπ metody jest, iø moøna jπ zastosowaÊ automatycznie, lecz jest zaleøna od danych analitycznych pochodzπcych z serwisu, ktÛre sπ niedoskona≥e.

Jak juø†zosta≥o opisane powyøej strona z artyku≥em tekstowym zawiera odnoúiki do innych artyku≥Ûw poruszajπcych tematykÍ podobnπ do danego. Skoro zapisywana jest informacja o przejúciach pomiÍdzy podstronami serwisu, to moøna policzyÊ ile razy z artyku≥u X dokonano przejúcia na rekomendowany do niego artyku≥ Y1, a ile razy na rekomendowany artyku≥ Y2. Jeøeli liczba przejúÊ na artyku≥ Y1 jest wiÍksza niø na Y2, moøna wnioskowaÊ, iø Y1 wydaje siÍ byÊ bardziej adekwatnπ rekomendacjπ dla artyku≥u X.

Pos≥ugujπc siÍ powyøszym za≥oøeniem, moøna zaproponowaÊ miarÍ jakoúci rekomendacji generowanych przez testowane metody w odniesieniu do popularnoúci rzeczywistych rekomendacji wyekstrahowanej z danych serwisu o aktywnoúci uøytkownikÛw.

W tym celu dokonujÍ adaptacji miary nDCG (Normalized Discounted Cumulative Gain). Miara ta s≥uøy do oceny jakoúci uszeregowania przedmiotÛw, np. wynikÛw zwracanych przez silniki wyszukiwania. 


\subsection{nDCG}
TUTAJ OPISUJ  MIAR  + podajÍ ürÛd≥o
%TODO PROBLEM - algorytm allegro zmienia≥ siÍ w czasie


\subsection{Adaptacja metody nDCG}
Za≥Ûømy, øe dany algorytm $A$ zwraca pewien ciπg artyku≥Ûw $c_A=a1,\ a2,\ ...,\ a6$ podobnych do danego artyku≥u $x$, w kolejnoúci od najbardziej adekwatnego. Za≥Ûømy rÛwnieø, czÍúÊ elementÛw ciπgu $c_B$ artyku≥Ûw rekomandowanych w serwisie dla $x$ (uøywanπ dotychczas w serwisie metodπ $B$) znajduje siÍ rÛwnieø w ciπgu $c_A$, tj. np. ISTNIEJ• TAKIE $a_i, a_j$ (i, j to indeksy w ciπgu $c_A$), øe naleøπ do $c_A$ i $c_B$. Za≥Ûømy ponadto, øe z danych o klikniÍciach uøytkownikÛw w linki w ramach serwisu wiadomo, øe przejúcie z $x$ na $a_i$ jest bardziej popularne niø przejúcie z $x$ na $a_j$. Stπd jeøeli $i<j$ ($i>j$), to jakoúÊ dzia≥ania metody $A$ jest dobra (z≥a), bo metoda ta generuje podobne artyku≥y w kolejnoúci zgodnej ze stopniem podobieÒstwa z artyku≥em bazowym, opartym o czÍstoúc przejúÊ uøytkownikÛw miÍdzy artyku≥ami. %TODO DO PRZEREDAGOWANIA!!!!

Za wagi metody nDCG przyjmujÍ liczby przejúÊ pomiÍdzy artyku≥ami, a samπ metodÍ stosujÍ tylko do przeciÍcia zbioru artyku≥Ûw podobnych do danego generowanych przez danπ metodÍ ze zborem artyku≥Ûw rekomendowanych do danego przez dotychczasowπ metodÍ dzia≥ajπcπ w serwisie.


\chapter{Opis i wyniki badaÒ}

\section{Przygotowanie eksperymentÛw}

Metodami, ktÛre aplikujÍ do problemu rekomendacji artyku≥Ûw sπ:
\begin{enumerate}
	\item[] [w2v\_wdnt\_centr] Word2Vec z modelem\cite{pias} uczonym na korpusie S≥owosieci\cite{wordnet} (model opisany jest poniøej) oraz odleg≥oúciami miÍdzy dokumentami liczonymi na bazie centroidu dokumentu.
	\item[] [w2v\_wdnt\_wmd] Word2Vec z modelem\cite{pias} uczonym na korpusie S≥owosieci oraz odleg≥oúciami miÍdzy dokumentami liczonymi metodπ Word Mover's Distance
	\item[] [w2v\_art\_centr] Word2Vec z modelem uczonym na korpusie oraz odleg≥oúciami miÍdzy dokumentami liczonymi na bazie centroidu dokumentu.
	\item[] [w2v\_art\_wmd] Word2Vec z modelem uczonym na korpusie oraz odleg≥oúciami miÍdzy dokumentami liczonymi metodπ Word Mover's Distance
	\item[] [lda] Latent Dirichlet Allocation
\end{enumerate}

Ponadto wyniki zastosowania powyøszych metod porÛwnujÍ z dotychczasowπ metodπ wykorzystywanπ w serwisie allegro [allegro] oraz z losowπ ocena podobieÒstwa artyku≥Ûw [random].

\subsection{Modele Word2Vec}

\subsubsection{Model uczony na korpusie S≥owosieci}

Jako podstawowy model Word2vec uøy≥em gotowego modelu\cite{pias} stworzonego m.in. przez dr inø. M. Piaseckiego. Model ten by≥ uczony na korpusie S≥owosieci wer. 10. Dane przed uczeniem  przesz≥y segmentacjÍ, lematyzacjÍ i ujednoznacznianie morfosyntaktyczne. Uøyte parametry uczenia Word2Vec: metoda skip gram, wekroty d≥ugoúci 100, okno kontekstu wielkoúci 5.

Model ten zawiera 73875 spoúrÛd 98174 (75\%) unikalnych s≥Ûw oraz 7313915 z 7409145 (99\%) wszystkich s≥Ûw korpusu artyku≥Ûw. Wskazuje to, iø s≥owa nieobecne w modelu sπ bardzo ma≥o popularne w korpusie artyku≥Ûw (stanowiπ ok 1\% ca≥oúci). Po samodzielnym sprawdzeniu stwierdzam, øe s≥owa nieobecne w modelu to: ,,literÛwki'' lub s≥owa niepoprawnie stokenizowane (np. ,,urzπdzeia''), symbole marek produktÛw (np. ,,ux305fa'', ,,i7-4700qm''), øargon branøowy (np. ,,bootsÛw''), z≥oøenia wyrazÛw (np. ,,kurzoodporne''), wyrazy obce lub ich spolszczenia (np. ,,thermoprotect''). Uwaøam, iø mimo niewielkiej liczby tych s≥Ûw w stosuku mogπ mieÊ one znaczπcy wp≥yw na semantykÍ artyku≥Ûw.
\subsubsection{Model uczony na korpusie artyku≥Ûw}
W zwiπzku z powyøszym stwierdzeniem wykonujÍ naukÍ modelu Word2vec na korpusie artyku≥Ûw w celu zawarcia brakujπcych w poprzednim modelu s≥Ûw. Najrozsπdniejszym postÍpowaniem by≥oby tutaj rozszerzenie modelu opartego na korpusie S≥owosieci rÛwnieø o brakujπce s≥owa, jednak metoda Word2vec nie pozwala na dodanie nowych s≥Ûw do s≥ownika istniejπcego modelu, a jedynie na dalszπ naukÍ w oparciu o s≥owa juø istniejπce w s≥owniku.

Si≥π rzeczy model ten zawiera wszystkie s≥owa zawarte w korpusie artyku≥Ûw.

Do uczenia uøy≥em parametrÛw identycznych, jak w metodzie powyøej.

%ILE TRWA£O BUDOWANIE MODELU

\subsection{Model LDA}

%Przed wykonaniem ostatecznych pomiarÛw naleøy zbudowaÊ model LDA w oparciu o przetwarzane dane. W moim przypadku trwa≥o to: CZASY BUDOWANIA LDA

\section{Wyniki badaÒ}

Do kaødej z wymienionych powyøej metod stosujÍ kaødπ z trzech opracowanych przez mnie, opisanych wczeúniej miar : opartπ na kategoriach [categories], ocenach uøytkownikÛw offline [users] i klikniÍciach prawdziwych uøytkownikÛw serwisu [clicks]. Wyniki zestawiam w tabelce.

\begin{center}
	\begin{tabular}{ | l | l | l | p{5cm} |}
		\hline
		Alias metody & clicks & categories & users \\ \hline
		random & - & 0.145630871396 & \\ \hline
		w2v\_art\_wmd & 1.29748226281 & 0.542820699708 & \\ \hline
		wv2\_art\_centr & 1.37170817357 & 0.521784904438 & \\ \hline
		w2v\_wdnt\_wmd & 1.3817891755  & 0.54587787496 & \\ \hline
		w2v\_wdnt\_centr & 1.43155208802 & 0.517755911889 & \\ \hline
		lda & 1.56573051337 & 0.493703433754 & \\ \hline
		allegro & 1.14732593575 & 0.580296404276 & \\ \hline
	\end{tabular}
\end{center}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{img/results.png}
	\caption{PorÛwnanie znormalizowanych wynikÛw.}
	%\label{fig:warstwy}
\end{figure}

%TODO PopatrzeÊ w podsumowania innych prac

\subsection{EfektywnoúÊ czasowa}

Wadπ metody WMD wykluczajπcπ jπ z uøycia w tym przypadku jest jej powolnoúÊ. Z≥øonoúÊ czasowa metody wynosi:
Dla przypadku: [w2v\_art\_wmd] obliczenia trwa≥y 55 minut, co przy czasie <1sek dla centroidu jest wartoúcia niedopuszczalnπ. Proporcjonalnie uøycie tej metody dla ca≥ego korpusu trwa≥oby ok. 183 godzin.

Istotnπ zaletπ dotychczasowego rozwiπzainia stosowanego w Allegro jest uniwersalnoúÊ silnika elasticsearch oraz to, øe pozwala edytowaÊ inteksowane dane w locie, bez koniecznoúci przebudowy systemu. Metody LDA oraz Word2vec potrzebujπ przebudowania modelu przy kaødej zmianie korpusu, na ktÛrym siÍ opierajπ. 

\chapter{Podsumowanie}
Dalsze badania.

Niniejsza praca nie wyczerpuje sposobÛw wyboru artyku≥Ûw podobnych. 

Nie wszystkie pola zawarte w strukturze zosta≥y wykorzystane. Pozostajπ np. ,,autor''.

Przed zastosowaniem metod wyznaczania podobieÒstwa wykona≥em przetwarzanie wstÍpne dokumentÛw, ktÛre moøna przeprowadziÊ rÛwnieø na inne sposoby. Jest to temat osobnych badaÒ.

ZdajÍ sobie sprawÍ z niedoskona≥oúci zastosowanych miar.

Tematem niniejszej pracy jest przypisanie danemu artyku≥owi artyku≥Ûw najbardziej podobnych. Warto tutaj zaznaczyÊ rÛønicÍ pomiÍdzy tematykπ pracy a komercyjnym zagadnieniem najlepszych rekomendacji. Artyku≥y, ktÛre moøna uznaÊ za dobre rekomandacje, tj. takie, ktÛre przynoszπ przedsiÍbiorstwu najwiÍkszy zysk, wcale nie musø byÊ podobne do danego. Powszechnym zjawiskiem jest wzbogacanie rekomandacji o przedmioty niepodobne do danego, a pozwalajπce uøytkownikowi na poznanie osobnej kategorii przedmiotÛw, ktÛra moøe go zainteresowaÊ a tym samym przyciπgnπÊ do serwisu.


%-----------Koniec czÍúci zasadniczej-----------
\appendix
\chapter{Technologie i narzÍdzie}
%TODO przypisy
AnalizÍ danych, ich wstÍpne przetworzenie a nastÍpnie przeprowadzenie docelowych eksperymentÛw wykona≥em korzystajπc g≥Ûwnie z jÍzyka Python i szeregu skrytpÛw napisanych w nim w≥asnorÍcznie, wykorzystujπcych istniejπce specjalistyczne biblioteki posiadajπce interfejs w tymøe jÍzyku.

Wykorzystane narzÍdzia:
\begin{itemize}
	\item Elasticsearch - silnik wyszukiwania tekstowego. Uøywam go do przechowywania bazy artyku≥Ûw oraz ich przetworzonych wersji.
	\item MongoDB - nierelacyjna baza danych, ktÛrej uøywam do przechowywania wynikÛw generowanych przez testowane algorytmy.
\end{itemize}

Wykorzystane biblioteki jÍzyka Python:
\begin{itemize}
	\item Gensim - rozbudowana biblioteka s≥uøπca do przetwarzania jÍzyka naturalnego. Zawiera implementacjÍ metod Word2Vec, LDA, TF-IDF i inne.
	\item Morfologik - tokenizer jÍzyka polskiego
	\item Numpy - pozwala wydajnie wykonywaÊ obliczenia numeryczne
	\item Pyemd - implementacja algorytmu Earth Mover's Distance
	\item Elasticsearch - u≥atwia wykonywanie zapytaÒ do silnika Elasticsearch wprost z kodu Pythona
	\item Matplotlib - biblioteka s≥uøπca do wykonywania wykresÛw
	\item Pymongo - umoøliwia wykonywanie zapytaÒ do bazy MongoDB wprost z kodu Pythona
\end{itemize} 


\begin{thebibliography}{11}
%TODO uporzπdkowaÊ bibliografiÍ, dodaÊ przecinki
\bibitem[1]{handbook}
	Francesco Ricci, Lior Rokach, Bracha Shapira,
	\emph{Introduction to Recommender Systems Handbook},
	Springer,
	2011
\bibitem[2]{slownik}
	S≥ownik JÍzyka Polskiego PWN
	\url{http://sjp.pwn.pl/sjp/artykul;2441396.html}
	(07.05.2017)
\bibitem[3]{allegro}
	\url{https://magazyn.allegro.pl/3333-serwis-allegro-to-nasz
	-sposob-na-wasze-szybkie-i-wygodne-zakupy-przez-internet}
	(07.05.2017)
\bibitem[4]{morfologik}
	\url{http://morfologik.blogspot.com/}
	(07.05.2017)
\bibitem[5]{word2vec}
	Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean,
	\emph{Efficient Estimation of Word Representations in Vector Space},
	International Conference on Machine Learning (ICML),
	2013
\bibitem[6]{google_word2vec}
	\url{https://code.google.com/archive/p/word2vec/}
	(26.05.2017)
\bibitem[7]{word2vec_tutorial}
	\url{http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/}
	(26.05.2017)
\bibitem[9]{emd_method}
	Ofir Pele, Michael Werman,
	\emph{Fast and robust earth mover's distances},
	ICCV,
	2009
\bibitem[10]{emd}
	Yossi Rubner, Carlo Tomasi, and Leonidas J. Guibas,
	\emph{The Earth Mover's Distance as a Metric for Image Retrieval}
	str. 1,
	Computer Science Department, Stanford University,
	2000
\bibitem[12]{wmd}
	Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, Kilian Q. Weinberger,
	\emph{From Word Embeddings To Document Distances},
	International Conference on Machine Learning (ICML),
	2015
\bibitem[13]{softmax}
	\url{https://en.wikipedia.org/wiki/Softmax_function/}
	(11.06.2017)
\bibitem[14]{screen_allegro}
	\url{https://allegro.pl/artykul/jaka-farba-dla-alergika-55917/}
	(26.06.2017)
\bibitem[15]{stopwords}
	\url{https://pl.wikipedia.org/wiki/Wikipedia:Stopwords}
	(15.04.2017)
\bibitem[16]{pias}
	Pawe≥ KÍdzia, Gabriela Czachor, Maciej Piasecki, Jan KocoÒ
	\emph{Vector representations of polish words (Word2Vec method)}
	Wroc≥aw University of Technology
	2016
	\url{https://clarin-pl.eu/dspace/handle/11321/327}
	(26.06.2017)
\bibitem[17]{wordnet}
	\url{http://plwordnet.pwr.wroc.pl/wordnet/}
	(28.06.2017)
\bibitem[18]{glove_cran}
	\url{https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html}
	(30.08.2017)
\bibitem[19]{glove}
	Jeffrey Pennington, Richard Socher, Christopher D. Manning
	\emph{GloVe: Global Vectors for Word Representation}
	Computer Science Department, Stanford University, Stanford, CA 94305
	2014
\bibitem[20]{bow}
	Zellig Harris
	\emph{Distributional Structure}
	WORD, tom 10, num. 2-3
	1954
\bibitem[21]{firth}
	J.R. Firth.
	\emph{A synopsis of linguistic theory 1930-1955}
	Oxford: Philological Society
	1957
\bibitem[22]{lsa}
	Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, Richard Harshman
	\emph{Indexing by latent semantic analysis}
	Journal of the American Society for Information Science, tom 41, num. 6
	1990
\bibitem[23]{lda}
	David M. Blei, Andrew Y. Ng, Michael I. Jordan
	\emph{Latent Dirichlet Allocation}
	Journal of Machine Learning Research, tom 3 num. 4ñ5
	2003
\bibitem[24]{bengio}
	Bengio
\bibitem[25]{aylien}
	\url{http://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/}
	(30.08.2017)
\bibitem[26]{all_naj}
	\url{http://gadzetomania.pl/11824,zakupy-w-sieci-porownanie-najwiekszych-polskich-serwisow-aukcyjnych-2}
	(09.08.17)
\end{thebibliography}

\appendix
%\chapter{Instrukcja uøytkownika}
%\paragraph{}
\clearpage
\pagestyle{empty}
\noindent Warszawa, dnia ...............
\vspace{5cm}
\begin{center}
\LARGE{Oúwiadczenie}
\end{center}
Oúwiadczam, øe pracÍ magisterskπ pod tytu≥em: ,,Rekomendacje artyku≥Ûw opisujπcych produkty w serwisach e-commerce'', ktÛrej promotorem jest dr inø. Anna WrÛblewska, wykona≥em samodzielnie, co poúwiadczam w≥asnorÍcznym podpisem.
\vspace{2cm}
\begin{flushright}
...........................................
\end{flushright}
\end{document}