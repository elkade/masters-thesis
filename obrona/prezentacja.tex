\documentclass{beamer}
\usepackage[T1]{fontenc}

\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage[polish]{babel}
\usepackage{caption}
\mode<presentation>
{
	\usetheme{Warsaw}      % or try Darmstadt, Madrid, Warsaw, ...
	\usecolortheme{crane} % or try albatross, beaver, crane, ...
	\usefonttheme{default}  % or try serif, structurebold, ...
	\setbeamertemplate{headline}{}
	\setbeamertemplate{caption}[numbered]
} 

\title[Praca magisterska]{Rekomendacje artykułów opisujących produkty w serwisach e-commerce}
\author{Łukasz Dragan}
\institute{Informatyka spec. Metody sztucznej inteligencji, MiNI PW}
\date{31.10.2017}

\AtBeginSection[]{
	\begin{frame}
		\vfill
		\centering
		\begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
			\usebeamerfont{title}\insertsectionhead\par%
		\end{beamercolorbox}
		\vfill
	\end{frame}
}

\begin{document}
	
	\begin{frame}
		\titlepage
	\end{frame}
	
	\begin{frame}{Plan prezentacji}
	  \tableofcontents
	\end{frame}
	
	\section{Opis problemu}
	
	\begin{frame}{Cel pracy}
		Czy metody semantycznej analizy tekstu mogą być alternatywą dla~dotychczas używanej przez~\emph{Allegro} metody generowania rekomendacji artykułów tekstowych?
		%powiedzeić o swojej motywacji: nie znałem wcześniej tej tematyki, nie znałem Pani Promotor, temat wydał mi się ciekawy, bo było to realne biznesowe zastosowanie informatyki. Ostatecznie otworzyło to przede mną szerokie pole do dalszego zainteresowania tematem. A i współpraca okazała się owocna
	\end{frame}
	
	\begin{frame}{pracuj.pl}
		\begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{img/pracuj.png}
		\end{figure}
	\end{frame}
	
	\begin{frame}{filmweb.pl}
		\begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{img/filmweb.png}
		\end{figure}
	\end{frame}
	
	\begin{frame}{allegro.pl}
		\begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{img/screen_allegro.png}
		\end{figure}
	\end{frame}
	
	\begin{frame}{allegro.pl cd}
		\begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{img/screen_allegro_2.png}
		\end{figure}
	\end{frame}
	
	\section{Systemy rekomendacji}
	\begin{frame}{Elasticsearch}
		\begin{figure}
			\centering
			\includegraphics[width=0.5\textwidth]{img/elastic-logo.png}
		\end{figure}
		,,Elasticsearch is a distributed, JSON-based search and analytics engine designed for horizontal scalability, maximum reliability, and easy management.''
	\end{frame}

	\begin{frame}{Systemy rekomenadacji}
		%TODO
		W ujęciu ogólnym systemy wyszukiwania mają na~celu sugerowanie tego, co użytkownik chciałby otrzymać. Natomiast systemy rekomendacji mają sugerować przedmioty potrzebne użytkownikowi nawet, jeżeli potrzeby te nie~zostały bezpośrednio wyrażone.
	\end{frame}
	
	\begin{frame}{Systemy rekomenadacji}
		\begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{img/recommender.png}
		\end{figure}
	\end{frame}
	\section{Techniki przetwarzania języka naturalnego}
	\begin{frame}{Zarys podejścia}
		\begin{figure}
			\centering
			\includegraphics[width=0.85\textwidth]{img/approach_outline.png}
		\end{figure}
		\begin{figure}
			\centering
			\includegraphics[width=0.45\textwidth]{img/scatter3d_demo.png}
		\end{figure}
	\end{frame}
	\begin{frame}{Dystans między wektorami}
		\begin{figure}
			\centering
			\includegraphics[width=0.6\textwidth]{img/cos.png}
		\end{figure}% dyatans kosinusowy zwraca od razu znormalizowany wynik
		\begin{equation}
		\label{eq:cos}
		sim=\cos(\theta )={\frac {\sum \limits _{i=1}^{n}{A_{i}B_{i}}}{{\sqrt {\sum \limits _{i=1}^{n}{A_{i}^{2}}}}{\sqrt {\sum \limits _{i=1}^{n}{B_{i}^{2}}}}}},
		\end{equation}
		gdzie $A_i$ i $B_i$ są składowymi wektorów $A$ i $B$
	\end{frame}
	\begin{frame}{Bag-of-words}
		\begin{figure}
			\centering
			\includegraphics[width=0.75\textwidth]{img/bow_sents.png}
		\end{figure}
		\begin{figure}
			\centering
			\includegraphics[width=0.25\textwidth]{img/bow_dict.png}
		\end{figure}
		\begin{figure}
			\centering
			\includegraphics[width=0.5\textwidth]{img/bow_repr.png}
		\end{figure}
		% Wadą jest traktowanie każdego słowa z~jednakową wagą
	\end{frame}
	\begin{frame}{TF – term frequency, IDF – inverse document frequency}
		Wartość \textit{TF-IDF} słowa $w_i$ w dokumencie $d_j$:
		\begin{equation}
		\label{eq:tf-idf}
		tfidf_{ij} = tf_{ij} * idf_i,\ tf_{ij} = \frac{n_{ij}}{\sum\limits_{k}n_{kj}},\ idf_i = log\frac{|D|}{|{d:w_i \in d}|}
		\end{equation}
		\begin{itemize}
			\item $tf_{ij}$: liczba wystąpień słowa $w_i$ w~dokumencie $d_j$ podzielona przez~liczbę słów dokumentu $d_j$,
			\item $idf_i$: liczba dokumentów w~korpusie podzielona przez~liczbę dokumentów zawierających przynajmniej jedno wystąpienie słowa $w_i$.
		\end{itemize}
		% które słówa są premiowane, a które penalizowane
	\end{frame}
	\begin{frame}{TF-IDF}
		Zalety:
		\begin{itemize}
			\item prostota
		\end{itemize}
		Wady:
		\begin{itemize}
			\item duża wymiarowość wektorów
			\item wektory niemalże ortogonalne
		\end{itemize}
	\end{frame}
		%Distributional hypothesis --- ,,słowa występujące w~tym samym kontekście niosą ze~sobą podobne znaczenie.''
		
		%Kolejne, bardziej zaawansowane, omawiane tu metody opierają się na~tzw. distributional hypothesis --- hipotezie zakładającej, że słowa występujące w~tym samym kontekście niosą ze~sobą podobne znaczenie. Sprzyja to zastosowaniu metod algebry liniowej jako narzędzia obliczeniowego oraz sposobu reprezentacji tekstu. Podstawowe podejście polega na~zgromadzeniu informacji o~rozkładzie słów w~dokumentach w~postaci wielowymiarowych wektorów, a~następnie wyodrębnieniu podobieństw pomiędzy~tymi wektorami, które świadczyłyby o~pewnych powiązaniach między reprezentowanymi słowami.
	\begin{frame}{Latent semantic indexing (1988)}
		\begin{itemize}
			\item Redukcja wymiarowości macierzy wystąpień słów w~dokumentach
			\begin{center}
				\begin{tabular}{ | l | l | l | l | l | l | l |}
					\hline
					 & $d_1$ & $d_2$ & $d_3$ & $d_4$  & $d_5$  & $d_6$ \\ \hline
					statek & 1 & 0 & 1 & 0 & 0 & 0 \\ \hline
					łódź & 0 & 1 & 0 & 0 & 0 & 0 \\ \hline
					ocean & 1 & 1 & 0 & 0 & 0 & 0 \\ \hline
					podróż & 1 & 0 & 0 & 1 & 1 & 0 \\ \hline
					wycieczka & 0 & 0 & 0 & 1 & 0 & 1 \\
					\hline
				\end{tabular}
			\end{center}
			\item Hiperparametr: docelowa wymiarowość
		\end{itemize}
	\end{frame}
	\begin{frame}{Latent semantic indexing (1988)}
		\begin{itemize}
			\setlength\itemsep{3em}
			\item Rozkład według wartości osobliwych:
				\begin{equation}
				\label{eq:svd}
				A = U \Sigma V^T,\!
				\end{equation}
				$U$ i $V$ to macierze ortogonalne\\
				$\Sigma$ to macierz diagonalna, taka, że $\Sigma = diag(\sigma_i)$, gdzie $\sigma_{i}$, to~nieujemne wartości szczególne macierzy $A$.
				% Wartości osobliwe σi macierzy A sa definiowane jako σi = √λ_i, gdzie λ_i oznacza wartości własne macierzy A∗A.
				%macierz ortogonalna: U^-1 = U^T 
			\item 
			\{(statek), (łódź), (ocean)\} $\to$ \\\{(1.3452 $*$ statek $+$ 0.2828 $*$ łódź), (ocean)\}
		\end{itemize}
	\end{frame}
	\begin{frame}{Latent Dirichlet allocation (2003)}
		\begin{itemize}
			\item Automatyczne wykrywanie tematów zawartych w dokumentach
			\item Dokumenty jako mieszanki tematów
			\item Tematy jako rozkłady prawdopodobieństwa na zbiorze słów
			\item Hiperparametr: docelowa liczba tematów
		\end{itemize}
	\end{frame}
	\begin{frame}{Latent Dirichlet allocation (2003)}
		Algorytm --- próbkowanie Gibbsa:\\
		\begin{enumerate}
			\setlength\itemsep{2em}
			\item Przejdź przez każdy dokument i~losowo (zgodnie z~rozkładem Dirichleta) przypisz każde słowo dokumentu do~jednego z $T$ tematów.
			
			% Warto zauważyć, iż etap ten daje pierwsze	przybliżenie docelowej reprezentacji. W kolejnych krokach należy poprawiać to przybliżenie.
			
			% Cechy rozkładu sprawiają, że tak dobrane początkowe wartości parametrów modelu są zgodne z intuicją, że dokument pokrywa jedynie mały zestaw tematów, a temat zawiera najczęściej tylko mały zestaw słów.
			\item Dla każdego dokumentu $d$, dla~każdego słowa $w$ należącego do~$d$, dla każdego tematu $t$ oblicz: $p(t | d)$
			%czyli odsetek liczby słów w $d$, które są aktualnie przypisane do tematu $t$
			oraz oblicz $p(w | t)$
			% czyli odsetek liczby wystąpień słowa $w$, które są przypisane do~tematu $t$ w~skali całego korpusu.
			Przypisz słowu $w$ nowy temat poprzez losowanie z~prawdopodobieństwem $p(t_i|d)*p(w|t)$ dla~każdego tematu $t_i$.
		\end{enumerate}
		% After repeating the previous step a large number of times, you’ll eventually reach a roughly steady state where your assignments are pretty good. So use these assignments to estimate the topic mixtures of each document (by counting the proportion of words assigned to each topic within that document) and the words associated to each topic (by counting the proportion of words assigned to each topic overall).
		
	\end{frame}
	
	\begin{frame}{Rozkład Dirichleta}
		\begin{figure}
			\centering
			\includegraphics[width=0.5\textwidth]{img/dirichlet.png}
		\end{figure}
	\end{frame}
	
	\begin{frame}{Word embeddings}
		\begin{itemize}
			\item Osadzanie słów w przestrzeni wektorowej
			\item Uczenie nienadzorowane
			\item Niska wymiarowość wektorów
			\item Reprezentacja słów wraz z~zależnościami pomiędzy nimi
		\end{itemize}

		% Dzieje się to w opozycji do wcześniejszych podejść podobnych do bag-of-words — produkującego ogromne, rzadkie wektory, których wymiary równają się rozmiarowi		słownika, o który oparty jest model (rzędu setek tysięcy)
		\begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{img/linear-relationships.png}
		\end{figure}
		%https://www.tensorflow.org/images/linear-relationships.png
	\end{frame}
	\begin{frame}{Word2vec (2013)}
		% 	Model predykcyjny, który na podstawie kontekstu przewiduje słowo
		\begin{itemize}
		\item Płytka sieć neuronowa typu feed-forward
		\item Dwa podejścia:
		\begin{itemize}
			\item \textit{Continuous bag of words}: na~postawie okna $N$ sąsiednich słów sieć przewiduje słowo,
			\item \textit{skip-gram}: na~podstawie słowa sieć dokonuje predykcji $N$ sąsiednich słów.
		\end{itemize}
		
		\end{itemize}
		
\end{frame}

\begin{frame}
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\textwidth]{img/w2v_training_data.png}
		\captionsetup{labelformat=empty}
		\caption{http://mccormickml.com}
		%\label{fig:warstwy}
	\end{figure}
\end{frame}

\begin{frame}	
	
		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{img/skipgram_cbow.png}
			%\label{fig:warstwy}
		\end{figure}
	\end{frame}
		%	\item WEktory kodowane są jako one-hot-vector: dla i-tego słowa 1 na i-tym miejscu
%			\item hiperparametry: gługość wektorów word embeddings = wielkość warstwy ukrytej; wielkość okna skanującego
	

	%	zaletą jest, że szybko się uczy, w przeciwieństwie do wcześniejszych podejść.
		


		
		
	%	x to kontekst słowa - szerokość ustalana hiperparametrem
%		y to słowo
		
%		Zamiast funkcji aktywacji Softmax
%		Softmax przekształca wyjście sieci w rozkład prawdopodobieństwa
%		Rozkład ten wskazuje, które słowo jest najbardziej prawdopodobne dla wejściowego kontekstu
%		wektory słów są efektem ubocznym nauki sieci
%		są to wagi połączeń między neuronami
	\begin{frame}{FastText (2017)}
		\begin{itemize}
			\item Rozwinięcie metody word2vec
			\item Rozbija słowa na n-gramy, np.
			\emph{pokój}$to$\emph{pok, okó, kój}
			\item Wektor wynikowy = wektor dla słowa + wektory jego n-gramów 
			\item Dobre wyniki dla języków bogatych morfosyntaktycznie, np. polskiego, tureckiego, czy fińskiego.
		\end{itemize}
%Podejście takie daje szereg nowych możliwości. Pomaga wyznaczyć reprezentację wektorową rzadkich słów, które być może mają wspólny rdzeń (i znaczenie) z innymi, częściej występującymi słowami. Metoda pozwala również nadać wektorową reprezentację słowom, których w ogóle nie ma w słowniku, ponieważ ich podsłowa mogą należeć do słów w słowniku się znajdujących. Zalety te wydają się być szczególnie obiecujące w przypadku bogatych morfologicznie języków, np. języka polskiego, tureckiego, czy fińskiego.
	\end{frame}
	\begin{frame}{GloVe --- Global Vectors (2014)}

		% w funkcji celu najniższą wartość osiągniemy, gdy w1 * w2 będzie równe log(Xij) - jeżeli wektory są blisko siebie to iloczyn jest duży. jeżeli włowa są częstow w swoim kontekście to Xij jest duże.

\begin{enumerate}
	\item Zgromadź współwystąpienia słów w formie globalnej macierzy $X$ takiej, że
	$X_{ij}$: ile razy słowo $w_i$ występuje w kontekście słowa $w_j$
	% Każdy element $X_{ij}$ takiej macierzy reprezentuje jak często słowo $i$ występuje w pobliżu słowa $j$. Zazwyczaj macierz buduje się poprzez skanowanie bazowego korpusu oknem o ustalonej szerokości, w obrębie którego centralne słowo leży w kontekście słów je otaczających. Dodatkowo można tu wprowadzić wagi dla słów malejące wraz ze wzrostem dystansu od słowa centralnego.

	\item Zdefiniuj ograniczenie dla każdej pary słów: 
	\begin{equation}
	w_i^Tw_j + b_i + b_j = log(X_{ij}),
	\end{equation}
	gdzie $w_i$ i $w_j$ to wektory odpowiadające słowom oraz $b_i$ i $b_j$ to skalary.

	\item Dokonaj minimalizacji funkcji kosztu:
	\begin{equation}
	\label{eq:glove_loss}
	J = \sum_{i=1}^V \sum_{j=1}^V \; f(X_{ij}) ( w_i^T w_j + b_i + b_j - \log X_{ij})^2,
	\end{equation}
	gdzie $f$ jest funkcją ważącą, $V$ to słownik.
	%która pomaga zapobiec uczeniu tylko na~podstawie najbardziej popularnych par słów.

	% Celem funkcji optymalizacji funkcji	 kosztu jest minimalizacja różnicy pomiędzy iloczynami skalarnymi wektorów współwystępujących słów.
\end{enumerate}
	\end{frame}

	\begin{frame}{Centroid}
		\begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{img/centroid.png}
		\end{figure}
	\end{frame}
	\begin{frame}{Word Mover's Distance}
		% Dystans pomiędzy dokumentami $A$ i $B$ to minimalny skumulowany dystans jaki słowa dokumentu $A$ muszą ,,przebyć'', aby osiągnąć słowa dokumnetu $B$
		\begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{img/wmd.png}
		\end{figure}
	\end{frame}
	\begin{frame}{Analiza danych}
		\begin{itemize}
			\item 20000~artykułów tekstowych w~formacie \textit{JSON}
			\item język polski
			\item słowa specyficzne dla różnych branż
			% ,,hipertoniczny'', ,,autofocus''
			\item struktura artykułu:
			\begin{itemize}
				\item treść: tytuł, nagłówek, tekst
				\item metadane: id, kategoria, słowa kluczowe
			\end{itemize}
			% w pracy opisuję dokładne statystyki
		\end{itemize}
	\end{frame}
	\section{Analiza danych}
	\begin{frame}{Wstępne przetwarzanie danych}
		\begin{enumerate}
			\item Oczyszczanie tekstu ze znaczników
			\item Usunięcie słów stopu
		\end{enumerate}
	\end{frame}
	\begin{frame}{Słowa stopu}
		a, aby, ach, acz, aczkolwiek, aj, albo, ale, ależ, ani, aż, bardziej, bardzo, bo, bowiem, by, byli, bynajmniej, być, był, była, było, były, będzie, będą, cali, cała, cały, ci, cię, ciebie, co, cokolwiek, coś, czasami, czasem, czemu, czy, czyli, daleko, dla, dlaczego, dlatego, do, dobrze, dokąd, dość, dużo, dwa, dwaj, dwie, dwoje, dziś, dzisiaj, gdy, gdyby, gdyż, gdzie, gdziekolwiek, gdzieś, go, i...
	\end{frame}
	\begin{frame}
		\begin{enumerate}
			\item Oczyszczanie tekstu ze znaczników
			\item Usunięcie słów stopu
			\item Zamiana na małe litery
			% większości duże litery na początku zdania przeszkadzają, ale czasem wyraz z dużej litery i z małej znaczą co innego - Włochy
			\item Tokenizacja i lematyzacja % morfologik.blogspot.com
		\end{enumerate}
	\end{frame}
	\begin{frame}{Preprocessing --- przykład}
		\begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{img/lemmatisation.png}
		\end{figure}
	\end{frame}
	% jaka byłaby sytuacja idealna? zaaplikować rozwiązania w żywym systemie i porównać kpi
	\section{Metody ewaluacji}
	% W celu porównania stosowanych metod wyznaczania podobieństwa między artykułami konieczna jest formalizacja pewnych miar tego podobieństwa. W opisie znanych ogólnych miar posłużyłem się pojęciem „relewantności” — formalną wartością wyrażoną za pomocą liczb rzeczywistych. W praktyce rzadko jednak dysponuje się wartością, na ile dany element rankingu jest adekwatny do zapytania generującego ów ranking.
	
	
	%!!!! WAŻNE opisane wcześniej metody przetwarzania języka naturalnego adaptuję do zadania generowania rekomendacji, tj. dla danego artykułu X mają one zwrócić sekwencję artykułów najbardziej do niego podobnych.
	\begin{frame}{Miara jakości wyszukiwania: Normalized Discounted Cumulative Gain}
		\begin{itemize}
			\item Discounted Cumulative Gain:
			\begin{equation}
			\label{eq:dcg}
			{\mathrm  {DCG_{{p}}}}=\sum _{{i=1}}^{{p}}{\frac  {rel_{i}}{\log _{{2}}(i+1)}},
			\end{equation}
			gdzie $p$ to liczba elementów rankingu, $i$ to miejsce przedmiotu w~rankingu, a $rel$ to poziom relewantności elementu.
			%% DCG premiuje relewantne przedmioty, które są wysoko w~rankingu oraz~karze za~relewantne przedmioty w~dole rankingu. W~wariancie nDCG następuje jeszcze normalizacja przez~podzielenie wartości DCG rzeczywistego rankingu przez DCG idealnego rankingu ($IDCG$, \ref{eq:idcg}) zbudowanego na~elementach korpusu ułożonych malejąco pod~kątem relewantności.
			
			% im bardziej relewantne wyniki, tym wyżej powinny być w~rankingu, aby ranking był najbardziej wartościowy

			\item Normalized Discounted Cumulative Gain:
			\begin{equation}
			\label{eq:ndcg}
			{\mathrm  {nDCG_{{p}}}}={\frac  {DCG_{{p}}}{IDCG_{{p}}}}.
			\end{equation}
		\end{itemize}
	\end{frame}
	\begin{frame}{Miary oparte na metadanych artykułów}
		\begin{itemize}
			\item Liczba wspólnych kategorii
					% Zaletą miary jest fakt, iż przypisanie artykułu do kategorii zostało wykonane przez autora, którego można określić ekspertem w dziedzinie tematyki artykułu
					
					% można ją zastosować automatycznie
			\item Liczba wspólnych słów kluczowych
			% relewantność wyszukanych artykułów liczona na podstawie liczby wspólnych słów kluczowych z artykułem bazowym.
		\end{itemize}
	\end{frame}
	\begin{frame}{Kliknięcia użytkowników serwisu}
		\begin{itemize}
			\item dodatkowe dane o kliknięciach z Allegro
			\item ocena na podstawie historycznej aktywności użytkowników
		\end{itemize}
	\end{frame}
	\begin{frame}{Ocena użytkowników offline}
		\begin{itemize}
			\item ekspercka ocena użytkowników
			\item 5 użytkowników oceniło po 300 par artykułów
		\end{itemize}
		%ocena na~podstawie eksperckiej oceny użytkowników. W~badaniu wykorzystałem 5~użytkowników operujących każdy na~tym samym zbiorze par testowych. Pary zostały wygenerowane (zgodnie z~wcześniejszym opisem metody) na~podstawie 50 artykułów bazowych wylosowanych spośród wszystkich artykułów udostępnionych mi przez~\textit{Allegro}.
	\end{frame}
	\section{Wyniki testów}
	\begin{frame}{LSI w zależności od liczby tematów}
		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{img/results/lsi_.png}
		\end{figure}
	\end{frame}
	\begin{frame}{LDA w zależności od liczby tematów}
		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{img/results/lda_.png}
		\end{figure}
	\end{frame}
	\begin{frame}{Word2vec w zależności od korpusu}
		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{img/results/w2v100_art_w2v100_wn_.png}
		\end{figure}
	\end{frame}
	\begin{frame}{Word2vec w zależności od długości wektorów}
		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{img/results/w2v_ctr.png}
		\end{figure}
	\end{frame}
	\begin{frame}{GloVe w zależności od długości wektorów}
		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{img/results/gv_ctr.png}
		\end{figure}
	\end{frame}
	\begin{frame}{FastText w zależności od długości wektorów}
		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{img/results/ft_ctr.png}
		\end{figure}
	\end{frame}
	\begin{frame}{Wyniki ewaluacji eksperckiej dla~wybranych metod}
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{img/results/lsi500_lda900_w2v300_gv1000_ft1000_allegro_rnd_users.png}
		\end{figure}
	\end{frame}
	\begin{frame}{Porównanie odchyleń standardowych ocen eksperckich dla~wybranych metod}
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{img/results/lsi500_lda900_w2v300_gv1000_ft1000_allegro_rnd_users_std.png}
		\end{figure}
	\end{frame}
	\section{Podsumowanie}
	\begin{frame}{Podsumowanie testów}
		\begin{itemize}
			\item Brak istotnych statystycznie różnic między wynikami wszystkich metod
			\item Im dłuższe wektory \emph{word embeddings}, tym lepsze rezultaty
			\item Większa liczba tematów nie implikuje lepszych rezultatów
		\end{itemize}
	\end{frame}
	\begin{frame}{Wnioski}
		\begin{itemize}
			\item Ewaluacja jest zadaniem nietrywialnym
			\item Nie każda biblioteka się nadaje
			\item Szybki rozwój dziedziny
			\item Wiele kierunków dalszych badań
			% inne sposoby preprocesingu, inne hiperparametry, inne metody porównywania dokumentów...
		\end{itemize}
	\end{frame}
	\section{Wybrane źródła}
	\begin{frame}{Wybrane źródła}
		\begin{thebibliography}{30}
			\bibitem{lda}
				D. M. Blei, A. Y. Ng, M. I. Jordan,
				\emph{Latent Dirichlet Allocation},
				Journal of Machine Learning Research, tom 3 num. 4–5,
				2003
			\bibitem{lsa}
				S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, R. Harshman,
				\emph{Indexing by latent semantic analysis},
				Journal of the American Society for Information Science, tom 41, num. 6,
				1990
			\bibitem{fasttext}
				A. Joulin, E. Grave, P. Bojanowski T. Mikolov,
				\emph{Bag of Tricks for Efficient Text Classification},
				Facebook AI Research,
				2016
			\bibitem{word2vec}
				T. Mikolov, K. Chen, G. Corrado, J. Dean,
				\emph{Efficient Estimation of Word Representations in Vector Space},
				International Conference on Machine Learning (ICML),
				2013
			\bibitem{glove}
				J. Pennington, R. Socher, C. D. Manning,
				\emph{GloVe: Global Vectors for Word Representation},
				Computer Science Department, Stanford University, Stanford, CA 94305,
				2014
		\end{thebibliography}
	\end{frame}
\end{document}
