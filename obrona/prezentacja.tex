\documentclass{beamer}
\usepackage[T1]{fontenc}

\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage[polish]{babel}
\usepackage{caption}
\usepackage{pgfpages}
\setbeameroption{show notes}
\setbeameroption{show notes on second screen=right}
\mode<presentation>
{
	\usetheme{Warsaw}      % or try Darmstadt, Madrid, Warsaw, ...
	\usecolortheme{crane} % or try albatross, beaver, crane, ...
	\usefonttheme{default}  % or try serif, structurebold, ...
	\setbeamertemplate{headline}{}
	\setbeamertemplate{caption}[numbered]
} 

\title[Praca magisterska]{Rekomendacje artykułów opisujących produkty w serwisach e-commerce}
\author{Łukasz Dragan}
\institute{Informatyka spec. Metody sztucznej inteligencji, MiNI PW}
\date{31.10.2017}
\AtBeginSection[]{
	\begin{frame}
		\vfill
		\centering
		\begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
			\usebeamerfont{title}\insertsectionhead\par%
		\end{beamercolorbox}
		\vfill
	\end{frame}
}

\begin{document}
	
	\begin{frame}
		\titlepage
	\end{frame}
	\note{
		\begin{itemize}
			\item 
		\end{itemize}
		}
	\begin{frame}{Plan prezentacji}
	  \tableofcontents
	\end{frame}
	\note{
		\begin{itemize}
			\item po kolei co zamierzam powiedzieć
		\end{itemize}
	}
	\section{Opis problemu}
	
	\begin{frame}{Cel pracy}
		Czy metody semantycznej analizy tekstu mogą być alternatywą dla~dotychczas używanej przez~\emph{Allegro} metody generowania rekomendacji artykułów tekstowych?

	\end{frame}
		\note{
			\begin{itemize}
				\item nie znałem wcześniej tej tematyki
				\item nie znałem Pani Promotor
				\item realne biznesowe zastosowanie informatyki
				\item zaznajomiłem się z dziedziną, której wcześniej nie znałem
			\end{itemize}
		}
	\begin{frame}{pracuj.pl}
		\begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{img/pracuj.png}
		\end{figure}
	\end{frame}
		\note{
			Każdy szanujący się serwis zawiera rekomendacje
		}
	\begin{frame}{filmweb.pl}
		\begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{img/filmweb.png}
		\end{figure}
	\end{frame}
	
	\begin{frame}{allegro.pl}
		\begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{img/screen_allegro.png}
		\end{figure}
	\end{frame}
			\note{
				\begin{itemize}
					\item w Allegro prócz głóœnej funkcjonalności
					\item artykuły opisujące produkty
					\item zawiera listę artykułów podobnych, którą można uznać za rekomendacje
				\end{itemize}
			}
	\begin{frame}{allegro.pl cd}
		\begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{img/screen_allegro_2.png}
		\end{figure}
	\end{frame}
		\begin{frame}{Elasticsearch --- dotychczas używana metoda}
			\begin{figure}
				\centering
				\includegraphics[width=0.5\textwidth]{img/elastic-logo.png}
			\end{figure}
			,,Elasticsearch is a distributed, JSON-based search and analytics engine designed for horizontal scalability, maximum reliability, and easy management.''
		\end{frame}
			\note{
				\begin{itemize}
					\item system rekomendacyjny zbudowany jest na elacticu
					\item elastic to silnik wyszukiwania
					\item zapytania opierają się o słówa kluczowe dołączone do artykułów jako metadane
					\item elstic jest szeroko wykorzystywany, ale nie dokonuje semantycznej analizy tekstu
				\end{itemize}
			}
	\section{Systemy rekomendacji}
	\note{
		Systemy wyszukiwania mają na~celu sugerowanie tego, co może się wydać użytkownikowi interesujące.
	}
	\begin{frame}{Systemy rekomenadacji}
		\begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{img/recommender.png}
		\end{figure}
	\end{frame}
	\note{
		\begin{itemize}
			\item kolaboratywne generuje rekomendacje na podstawie aktywności użytkowników o podobnym profilu
			\item jest niezależne od dziedziny
			\item .
			\item oparte na treści generuje rekomendacje oparte na tym, co w przeszłości podobało się użytkownikowi
			\item w tym przypadku na tym, co użytkownik właśnie przegląda
		\end{itemize}
	}
	\section{Techniki przetwarzania języka naturalnego}
	\begin{frame}{Zarys podejścia}
		\begin{figure}
			\centering
			\includegraphics[width=0.85\textwidth]{img/approach_outline.png}
		\end{figure}
		\begin{figure}
			\centering
			\includegraphics[width=0.45\textwidth]{img/scatter3d_demo.png}
		\end{figure}
	\end{frame}
	\note{
		\begin{itemize}
			\item Staramy się dokonać reprezentacji dokumentu w postaci wektora
			\item Po to, aby wykorzystać zależności między wektorami, np. odległość
			\item różne reprezentacje mają różną jakość
		\end{itemize}
	}
	\begin{frame}{Dystans między wektorami}
		\begin{figure}
			\centering
			\includegraphics[width=0.6\textwidth]{img/cos.png}
		\end{figure}% dyatans kosinusowy zwraca od razu znormalizowany wynik
		\begin{equation}
		\label{eq:cos}
		sim=\cos(\theta )={\frac {\sum \limits _{i=1}^{n}{A_{i}B_{i}}}{{\sqrt {\sum \limits _{i=1}^{n}{A_{i}^{2}}}}{\sqrt {\sum \limits _{i=1}^{n}{B_{i}^{2}}}}}},
		\end{equation}
		gdzie $A_i$ i $B_i$ są składowymi wektorów $A$ i $B$
	\end{frame}
		\note{\begin{itemize}
				\item odległości można mierzyć albo jako odległość euklidesową
				\item albo bardziej popularny sposób to kosinus kąta pomiędzy wektorami
			\end{itemize}}
	\begin{frame}{Bag-of-words}
		\begin{figure}
			\centering
			\includegraphics[width=0.75\textwidth]{img/bow_sents.png}
		\end{figure}
		\begin{figure}
			\centering
			\includegraphics[width=0.25\textwidth]{img/bow_dict.png}
		\end{figure}
		\begin{figure}
			\centering
			\includegraphics[width=0.5\textwidth]{img/bow_repr.png}
		\end{figure}
	\end{frame}
	\note{\begin{itemize}
			\item korpus to zbiór dokumentów, na których operujemy
			\item słownik to lista unikalnych słów
			\item najprostsze podejście --- worek słów
			\item dokument jako wektor z liczbą wystąpień i-tego słowa na itym miejscu
			\item .
			\item Wadą jest traktowanie każdego słowa z~jednakową wagą
			\item bardzo długie wektory
			\item wektory niemalże ortogonalne
		\end{itemize}}
	\begin{frame}{TF – term frequency, IDF – inverse document frequency}
		Wartość \textit{TF-IDF} słowa $w_i$ w dokumencie $d_j$:
		\begin{equation}
		\label{eq:tf-idf}
		tfidf_{ij} = tf_{ij} * idf_i,\ tf_{ij} = \frac{n_{ij}}{\sum\limits_{k}n_{kj}},\ idf_i = log\frac{|D|}{|{d:w_i \in d}|}
		\end{equation}
		\begin{itemize}
			\item $tf_{ij}$: liczba wystąpień słowa $w_i$ w~dokumencie $d_j$ podzielona przez~liczbę słów dokumentu $d_j$,
			\item $idf_i$: liczba dokumentów w~korpusie podzielona przez~liczbę dokumentów zawierających przynajmniej jedno wystąpienie słowa $w_i$.
		\end{itemize}
	\end{frame}
	\note{\begin{itemize}
			\item premiuje rzedkie słowa
		\end{itemize}}
	\begin{frame}{TF-IDF}
		Zalety:
		\begin{itemize}
			\item prostota
		\end{itemize}
		Wady:
		\begin{itemize}
			\item duża wymiarowość wektorów
			\item wektory niemalże ortogonalne
		\end{itemize}
	\end{frame}
	\note{\begin{itemize}
			\item prostota
			
			\item duża wymiarowość wektorów
			\item wektory niemalże ortogonalne
			
			\item zachowuje inne wady BOW
			\item bywa skłądową innych metod
		\end{itemize}}

	\begin{frame}{Latent semantic indexing (1988)}
		\begin{itemize}
			\item Redukcja wymiarowości macierzy wystąpień słów w~dokumentach
			\begin{center}
				\begin{tabular}{ | l | l | l | l | l | l | l |}
					\hline
					 & $d_1$ & $d_2$ & $d_3$ & $d_4$  & $d_5$  & $d_6$ \\ \hline
					statek & 1 & 0 & 1 & 0 & 0 & 0 \\ \hline
					łódź & 0 & 1 & 0 & 0 & 0 & 0 \\ \hline
					ocean & 1 & 1 & 0 & 0 & 0 & 0 \\ \hline
					podróż & 1 & 0 & 0 & 1 & 1 & 0 \\ \hline
					wycieczka & 0 & 0 & 0 & 1 & 0 & 1 \\
					\hline
				\end{tabular}
			\end{center}
			\item Hiperparametr: docelowa wymiarowość
		\end{itemize}
	\end{frame}
	\note{\begin{itemize}
			\item kolejne metody opierają się na hipotezie, że słowa występujące w~tym samym kontekście niosą ze~sobą podobne znaczenie
			\item w LSI budujemy macierz wystąpień słów w dokumentach
			\item następnie wykonujemy redukcję liczby wierszy do liczby podanej jako hiperparametr
			\item macierz zachowuje powiązania między słowami 
		\end{itemize}}
	\begin{frame}{Latent semantic indexing (1988)}
		\begin{itemize}
			\setlength\itemsep{3em}
			\item Rozkład według wartości osobliwych:
				\begin{equation}
				\label{eq:svd}
				A = U \Sigma V^T,\!
				\end{equation}
				$U$ i $V$ to macierze ortogonalne\\
				$\Sigma$ to macierz diagonalna, taka, że $\Sigma = diag(\sigma_i)$, gdzie $\sigma_{i}$, to~nieujemne wartości szczególne macierzy $A$.
				% Wartości osobliwe σi macierzy A sa definiowane jako σi = √λ_i, gdzie λ_i oznacza wartości własne macierzy A∗A.
				%macierz ortogonalna: U^-1 = U^T 
			\item 
			\{(statek), (łódź), (ocean)\} $\to$ \\\{(1.3452 $*$ statek $+$ 0.2828 $*$ łódź), (ocean)\}
		\end{itemize}
	\end{frame}
	\note{\begin{itemize}
			\item redukcja liczby wierszy za pomocą rozkładu wg. wartości osobliwych
			\item można potraktować jako grupowanie słów z odpowiednimi wagami
			\item ostatecznie zależnośic między dokumentami-wektorami są lepiej oddane
			\item wektory są krótkie
			\item jednak metoda ma nistą interpretowalność
		\end{itemize}}
	\begin{frame}{Latent Dirichlet allocation (2003)}
		\begin{figure}
			\centering
			\includegraphics[width=0.7\textwidth]{img/lda.jpg}
		\end{figure}
		\begin{itemize}
			\item Automatyczne wykrywanie tematów zawartych w dokumentach
			\item Dokumenty jako mieszanki tematów
			\item Tematy jako rozkłady prawdopodobieństwa na zbiorze słów
			\item Hiperparametr: docelowa liczba tematów
		\end{itemize}
	\end{frame}
	\note{\begin{itemize}
			\item Zadaniem metody jest reprezentacja dokumentów jako mieszanki tematów
			\item gdzie temat to mieszanka jawnych słów wybieranych z odpowiednimi wagami
			\item można to traktować jako redukcję wymiarowości
			\item liczba tematów określana hiperparametrem
		\end{itemize}}
	\begin{frame}{Rozkład Dirichleta}
		\begin{figure}
			\centering
			\includegraphics[width=0.5\textwidth]{img/dirichlet.png}
		\end{figure}
	\end{frame}
	\note{\begin{itemize}
			\item Początkowe przypisanie tematów do dokumentów i słów do tematów odbywa się zgodnie z roskładem Dirichleta
			\item zaóżmy że mamy 3 tematy, którym odpowiadają wierznhołki trójkąta
			\item liczba dokumentów składających się z tylko jednego tematu jest niska
			\item liczba dokumentów składających się z mieszanki tematów jest wysoka
			\item .
			\item te cechy rozkładu Dirichleta przyspieszają optymalizację
		\end{itemize}}
	\begin{frame}{Latent Dirichlet allocation (2003)}
		Algorytm --- próbkowanie Gibbsa:\\
		\begin{enumerate}
			\setlength\itemsep{2em}
			\item Przejdź przez każdy dokument i~losowo (zgodnie z~rozkładem Dirichleta) przypisz każde słowo dokumentu do~jednego z $T$ tematów.

			\item Dla każdego dokumentu $d$, dla~każdego słowa $w$ należącego do~$d$, dla każdego tematu $t$ oblicz: $p(t | d)$
			%czyli odsetek liczby słów w $d$, które są aktualnie przypisane do tematu $t$
			oraz oblicz $p(w | t)$
			% czyli odsetek liczby wystąpień słowa $w$, które są przypisane do~tematu $t$ w~skali całego korpusu.
			Przypisz słowu $w$ nowy temat poprzez losowanie z~prawdopodobieństwem $p(t_i|d)*p(w|t)$ dla~każdego tematu $t_i$.
		\end{enumerate}
		
\note{\begin{itemize}
		\item Rozkład daje pierwsze przybliżenie
		
		\item następnie iterujemy i przypisanie słów do tematów jest poprawiane
		
		\item dla każdego słowa w każdym dokumencie wyznaczany przypisywane jest temat, który jest najbardziej prawdopodobny na podstawie rozkładu w całości korpusu
		
		\item ostatecznie uzyskujemy w miarę stabilną sytuację, w której nie następują już zmiany przypisań słów do tematów
		
		\item podobne są dokumenty o podobnej proporcji przypisanych tematów
		
		\item metoda jest interpretowalne, gdyż wiemy, jakie słowa wchodzą w skłąd tematów
	\end{itemize}}
	\end{frame}
	\begin{frame}{Word embeddings}
		\begin{itemize}
			\item Osadzanie słów w przestrzeni wektorowej
			\item Uczenie nienadzorowane
			\item Niska wymiarowość wektorów
			\item Reprezentacja słów wraz z~zależnościami pomiędzy nimi
		\end{itemize}
		\begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{img/linear-relationships.png}
		\end{figure}
		%https://www.tensorflow.org/images/linear-relationships.png
	\end{frame}
\note{\begin{itemize}
		\item Kolejnym podejściem jest osadzanie słów w przestrzeni wektorowej, a następnie porównywanie dokumentów jako list takich wektorów
		
		\item celem metod tej grupy jest przypisanie słowom takich wektorów, żeby zachowywały one zależności zachodzące między słowami
	\end{itemize}}
%	\begin{frame}{Word2vec (2013)}
%		% 	Model predykcyjny, który na podstawie kontekstu przewiduje słowo
%		\begin{itemize}
%		\item Płytka sieć neuronowa typu feed-forward
%		\item Dwa podejścia:
%		\begin{itemize}
%			\item \textit{Continuous bag of words}: na~postawie okna $N$ sąsiednich słów sieć przewiduje słowo,
%			\item \textit{skip-gram}: na~podstawie słowa sieć dokonuje predykcji $N$ sąsiednich słów.
%		\end{itemize}
%		
%		\end{itemize}
		
%	\end{frame}

\begin{frame}{Word2vec (2013)}
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\textwidth]{img/w2v_training_data.png}
		\captionsetup{labelformat=empty}
		\caption{http://mccormickml.com}
		%\label{fig:warstwy}
	\end{figure}
\end{frame}
\note{\begin{itemize}
		\item Kluczowym reprezentantem podejścia jest word2vec
		\item używa płytkiej sieci neuronowej feed-forward do predykcji słów
		\item .
		\item zbiór uczący jest skaowany oknem o zadanej wielkości
		słowa z każdego okna są wejćiem i wyjściem sieci wektorowej
		\item kodujemy je jeko one-hot-wektor czyli dla i-tego słowa wektor zer z jedynką n i-tym miejscu
		
	\end{itemize}}
\begin{frame}	
	
		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{img/skipgram_cbow.png}
			%\label{fig:warstwy}
		\end{figure}
	\end{frame}
	
\note{\begin{itemize}
		\item wyróżniamy dwa podejścia CBOW: predykcja słowa na podstawie konekstu
		\item skip-gram: predykcja kontekstu na podstawie słowa
		
		\item sieć zamiast funkcji aktywacji ma funkcję softmax, która zamienia wyjście na rozkłąd prawdopodobieństwa
		
		\item ALE GDZIE TE WEKTORY? 
		\item wektory reprezentujące słowa są efektem ubocznym nauczonej sieci
		\item zawarte są w wagach warstwy ukrytej i mają długość równą rozmiarowi tej warstwy
		
		\item wcześniejsze podejścia opierały się na głębszych sieciach, które działały mało wydajnie
	\end{itemize}}
	\begin{frame}{FastText (2017)}
		\begin{itemize}
			\item Rozwinięcie metody word2vec
			\item Rozbija słowa na n-gramy, np.
			\emph{pokój} $\to$ \emph{pok, okó, kój}
			\item Wektor wynikowy = wektor dla słowa + wektory jego n-gramów 
			\item Dobre wyniki dla języków bogatych morfosyntaktycznie, np. polskiego, tureckiego, czy fińskiego.
		\end{itemize}
\note{\begin{itemize}
		\item Jest modyfikacją word2vec
		\item powstało niedawno
		\item metoda rozbija słowo na n-gramy : podsłowa o określonej długości
		\item ostatecznie wektor słowa to suma wektoru słowa i wektorów podsłów
		\item sprawdza się dla języków bogatych morfosyntaktycznie
	\end{itemize}}
	\end{frame}
	\begin{frame}{GloVe --- Global Vectors (2014)}


\begin{enumerate}
	\item Zgromadź współwystąpienia słów w formie globalnej macierzy $X$ takiej, że
	$X_{ij}$: ile razy słowo $w_i$ występuje w kontekście słowa $w_j$

	\item Zdefiniuj ograniczenie dla każdej pary słów: 
	\begin{equation}
	w_i^Tw_j + b_i + b_j = log(X_{ij}),
	\end{equation}
	gdzie $w_i$ i $w_j$ to wektory odpowiadające słowom oraz $b_i$ i $b_j$ to skalary.

	\item Dokonaj minimalizacji funkcji kosztu:
	\begin{equation}
	\label{eq:glove_loss}
	J = \sum_{i=1}^V \sum_{j=1}^V \; f(X_{ij}) ( w_i^T w_j + b_i + b_j - \log X_{ij})^2,
	\end{equation}
	gdzie $f$ jest funkcją ważącą, $V$ to słownik.
	%która pomaga zapobiec uczeniu tylko na~podstawie najbardziej popularnych par słów.

\end{enumerate}
	\end{frame}
\note{\begin{itemize}
		\item podejście alternatywne dla word2vec
		\item operuje na globalnej macierzy współwystąpień słow
		\item w funkcji celu najniższą wartość osiągniemy, gdy w1 * w2 będzie równe log(Xij) - jeżeli wektory są blisko siebie to iloczyn jest duży. jeżeli włowa są częstow w swoim kontekście to Xij jest duże.
		
		\item Celem funkcji optymalizacji funkcji kosztu jest minimalizacja różnicy pomiędzy iloczynami skalarnymi wektorów współwystępujących słów.
		
		\item wyniki podobne do word2vec
	\end{itemize}}
	\begin{frame}{Centroid}
		\begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{img/centroid.png}
		\end{figure}
	\end{frame}
\note{\begin{itemize}
		\item Mamy już wektorowe reprezentacje słów wchodzących w skład dokumentu - CO DALEJ
		\item można obliczyć średnią tych wektorów - centroid, ale przy tym traci się część informacji
	\end{itemize}}
	\begin{frame}{Word Mover's Distance}
		\begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{img/wmd.png}
		\end{figure}
	\end{frame}
\note{\begin{itemize}
		\item Alternatywą jest Word Mover's Distance
		\item Dystans pomiędzy dokumentami $A$ i $B$ to minimalny skumulowany dystans jaki słowa dokumentu $A$ muszą ,,przebyć'', aby osiągnąć słowa dokumnetu $B$
		\item Metoda jest kosztowna obliczeniowo
	\end{itemize}}
	\section{Analiza danych}
	\begin{frame}{Analiza danych}
		\begin{itemize}
			\item 20000~artykułów tekstowych w~formacie \textit{JSON}
			\item język polski
			\item słowa specyficzne dla różnych branż
			% ,,hipertoniczny'', ,,autofocus''
			\item struktura artykułu:
			\begin{itemize}
				\item treść: tytuł, nagłówek, tekst
				\item metadane: id, kategoria, słowa kluczowe
			\end{itemize}
			% w pracy opisuję dokładne statystyki
		\end{itemize}
	\end{frame}
\note{\begin{itemize}
		\item 20000~artykułów tekstowych w~formacie \textit{JSON}
		\item język polski
		\item słowa specyficzne dla różnych branż
		% ,,hipertoniczny'', ,,autofocus''
		\item struktura artykułu:
		\begin{itemize}
			\item treść: tytuł, nagłówek, tekst
			\item metadane: id, kategoria, słowa kluczowe
		\end{itemize}
	\end{itemize}}
	\begin{frame}{Wstępne przetwarzanie danych}
		\begin{enumerate}
			\item Oczyszczanie tekstu ze znaczników
			\item Usunięcie słów stopu
		\end{enumerate}
	\end{frame}

	\begin{frame}{Słowa stopu}
		a, aby, ach, acz, aczkolwiek, aj, albo, ale, ależ, ani, aż, bardziej, bardzo, bo, bowiem, by, byli, bynajmniej, być, był, była, było, były, będzie, będą, cali, cała, cały, ci, cię, ciebie, co, cokolwiek, coś, czasami, czasem, czemu, czy, czyli, daleko, dla, dlaczego, dlatego, do, dobrze, dokąd, dość, dużo, dwa, dwaj, dwie, dwoje, dziś, dzisiaj, gdy, gdyby, gdyż, gdzie, gdziekolwiek, gdzieś, go, i...
	\end{frame}
	\begin{frame}
		\begin{enumerate}
			\item Oczyszczanie tekstu ze znaczników
			\item Usunięcie słów stopu
			\item Zamiana na małe litery
			\item Tokenizacja i lematyzacja % morfologik.blogspot.com
		\end{enumerate}
\note{\begin{itemize}
		\item w większości duże litery na początku zdania przeszkadzają, ale czasem wyraz z dużej litery i z małej znaczą co innego - Włochy
		\item lematyzacja to najistotniejszy element --- sprowadza słowa do postaci podstawowej
		\item np jest, była, będzie do ,,być''
		\item używam polskiego narzędzi Morfologik
	\end{itemize}}
	\end{frame}
	\begin{frame}{Preprocessing --- przykład}
		\begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{img/lemmatisation.png}
		\end{figure}
	\end{frame}
	% jaka byłaby sytuacja idealna? zaaplikować rozwiązania w żywym systemie i porównać kpi
	\section{Metody ewaluacji}
	\begin{frame}{Miary jakości wyszukiwania}
		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{img/ranking.png}
		\end{figure}
	\end{frame}
	
\note{\begin{itemize}
		\item W swojej pracy chcę sprawdzić, czy da się zastosować metody semantycznej analizy tekstu do generowania rekomendacji w allegro
		\item w tym celu dokonuję adaptacji opisanych wcześniej metod
		\item na wejściu daję artykuł i oczekuję listy rekomendacji
		\item .
		\item Pojęcie relewantności --- jak bardzo artykuł zarekomandowany jest związany z tym bazowym
		\item przykład: przepis na szarlotkę i oleje silnikowe są mało relewantne
	\end{itemize}}
	
	\begin{frame}{Miary jakości wyszukiwania}
		\begin{itemize}
			\item Średnia relewantność
			\item Discounted Cumulative Gain:
			\begin{equation}
			\label{eq:dcg}
			{\mathrm  {DCG_{{p}}}}=\sum _{{i=1}}^{{p}}{\frac  {rel_{i}}{\log _{{2}}(i+1)}},
			\end{equation}
			gdzie $p$ to liczba elementów rankingu, $i$ to miejsce przedmiotu w~rankingu, a $rel$ to poziom relewantności elementu.
		\end{itemize}
	\end{frame}
	\note{\begin{itemize}
			\item W celu oceny jakości rankingu można obliczyć jego średnią relewantność
			\item Albo można użyć miary DCG, która bierze pod uwagę kolejność wyników
			\item im bardziej relewantne wyniki, tym wyżej powinny być w~rankingu, aby ranking był najbardziej wartościowy
			\item JEDNAKŻE!!
			\item Relewantność: W praktyce rzadko jednak dysponuje się wartością, na ile dany element rankingu jest adekwatny do zapytania generującego ów ranking.
		\end{itemize}}
	
	\begin{frame}{Miary oparte na metadanych artykułów}
		\begin{itemize}
			\item Liczba wspólnych kategorii
			\item Liczba wspólnych słów kluczowych
		\end{itemize}
	\end{frame}
	\note{\begin{itemize}
			\item relewantność wyszukanych artykułów liczona na podstawie liczby wspólnych słów kluczowych z artykułem bazowym.
			\item Zaletą miar jest fakt, iż przypisanie artykułu do kategorii zostało wykonane przez autora, którego można określić ekspertem w dziedzinie tematyki artykułu
			\item można ją zastosować automatycznie
		\end{itemize}}
	\begin{frame}{Kliknięcia użytkowników serwisu}
		\begin{itemize}
			\item dodatkowe dane o kliknięciach z Allegro
			\item ocena na podstawie historycznej aktywności użytkowników
		\end{itemize}
	\end{frame}
\note{\begin{itemize}
		\item Allegro zbiera w logach informację o kliknięciach użytkowników w linki
		\item skoro każdy artykuł posiada rekomendacje wygenerowane dotychczasową metodą to na podstawie liczby przejść na poszczególne rekomendacje można określić, która z nich jest najbardziej popularna
		\item tę popularność --- relewantność można zestawić z wynikami moich adaptacji metod 
	\end{itemize}}
	\begin{frame}{Ocena użytkowników offline}
		\begin{itemize}
			\item ekspercka ocena użytkowników
			\item 5 użytkowników oceniło po 300 par artykułów
		\end{itemize}
	\end{frame}
	\note{\begin{itemize}
			\item Do badania wygenerowałem każdą z wybranych metod rekomendacje do wylosowanych 50 artykułów
			\item podzieliłem je na pary artykuł bazowy --- artykuł rekomendowany
			\item zbudowałem lokalny interfejs webowy do ocenty tych par
			\item zaangażowałem 5 użytkowników, aby dokonali oceny, jak bardzo podobne są artykuły w poszczególnych parach
		\end{itemize}}
	
	\section{Wyniki testów}
\note{\begin{itemize}
		\item Ostatecznie testuję wybrane metody w zależności od ich hiperparametrów
		\item do ewaluacji stosuję opisane miary
		\item wszystkie wyniki są znormalizowane względem najlepszego wyniku w danym teście
	\end{itemize}}
	\begin{frame}{LSI w zależności od liczby tematów}
		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{img/results/lsi_.png}
		\end{figure}
	\end{frame}
\note{\begin{itemize}
		\item Co oznaczają skróty
		\item wyniki rosną a potem przestają
		\item związek między ndcg i średnią
	\end{itemize}}
	\begin{frame}{LDA w zależności od liczby tematów}
		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{img/results/lda_.png}
		\end{figure}
	\end{frame}
\note{\begin{itemize}
		\item podobna sytuacja
		\item metoda oparta o klikięcia dziwnie się zachowuje - nie widać tendencji
	\end{itemize}}
	\begin{frame}{Word2vec w zależności od długości wektorów}
		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{img/results/w2v_ctr.png}
		\end{figure}
	\end{frame}
\note{\begin{itemize}
		\item testowałem WMD, ale działał zbyt wolno. Używam centroidu
		\item wyniki bardzo do siebie zbliżone
	\end{itemize}}
	\begin{frame}{GloVe w zależności od długości wektorów}
		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{img/results/gv_ctr.png}
		\end{figure}
	\end{frame}
\note{\begin{itemize}
		\item duże różnice
		\item im dłuższe wektory tym lepiej
	\end{itemize}}
	\begin{frame}{FastText w zależności od długości wektorów}
		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{img/results/ft_ctr.png}
		\end{figure}
	\end{frame}
	\begin{frame}{Zestawienie wyników}
		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{img/results/lsi500_lda900_w2v300_gv1000_ft1000_allegro_rnd_.png}
		\end{figure}
	\end{frame}
	\begin{frame}{Wyniki ewaluacji eksperckiej dla~wybranych metod}
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{img/results/lsi500_lda900_w2v300_gv1000_ft1000_allegro_rnd_users.png}
		\end{figure}
	\end{frame}
	\begin{frame}{Porównanie odchyleń standardowych ocen eksperckich dla~wybranych metod}
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{img/results/lsi500_lda900_w2v300_gv1000_ft1000_allegro_rnd_users_std.png}
		\end{figure}
	\end{frame}
	\section{Podsumowanie}
	\begin{frame}{Podsumowanie testów}
		\begin{itemize}
			\item Brak istotnych statystycznie różnic między wynikami wszystkich metod
			\item Im dłuższe wektory \emph{word embeddings}, tym lepsze rezultaty
			\item Większa liczba tematów nie implikuje lepszych rezultatów
		\end{itemize}
	\end{frame}
\note{\begin{itemize}
		\item metoda oparta o kliknięcia okazała się słaba
	\end{itemize}}
	\begin{frame}{Wnioski}
		\begin{itemize}
			\item Ewaluacja jest zadaniem nietrywialnym
			\item Nie każda biblioteka się nadaje
			\item Szybki rozwój dziedziny
			\item Wiele kierunków dalszych badań
		\end{itemize}
	\end{frame}
\note{\begin{itemize}
		\item inne sposoby preprocesingu
		\item inne hiperparametry
		\item inne metody porównywania dokumentów - np testy A/B
	\end{itemize}}
	\section{Wybrane źródła}
	\begin{frame}{Wybrane źródła}
		\begin{thebibliography}{30}
			\bibitem{lda}
				D. M. Blei, A. Y. Ng, M. I. Jordan,
				\emph{Latent Dirichlet Allocation},
				Journal of Machine Learning Research, tom 3 num. 4–5,
				2003
			\bibitem{lsa}
				S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, R. Harshman,
				\emph{Indexing by latent semantic analysis},
				Journal of the American Society for Information Science, tom 41, num. 6,
				1990
			\bibitem{fasttext}
				A. Joulin, E. Grave, P. Bojanowski T. Mikolov,
				\emph{Bag of Tricks for Efficient Text Classification},
				Facebook AI Research,
				2016
			\bibitem{word2vec}
				T. Mikolov, K. Chen, G. Corrado, J. Dean,
				\emph{Efficient Estimation of Word Representations in Vector Space},
				International Conference on Machine Learning (ICML),
				2013
			\bibitem{glove}
				J. Pennington, R. Socher, C. D. Manning,
				\emph{GloVe: Global Vectors for Word Representation},
				Computer Science Department, Stanford University, Stanford, CA 94305,
				2014
		\end{thebibliography}
	\end{frame}
	\begin{frame}{}
		Dziękuję za uwagę
	\end{frame}
\end{document}
